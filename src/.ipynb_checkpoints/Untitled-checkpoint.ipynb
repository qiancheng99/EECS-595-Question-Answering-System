{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64eb0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    squad_convert_examples_to_features,\n",
    ")\n",
    "from transformers.data.metrics.squad_metrics import (\n",
    "    compute_predictions_log_probs,\n",
    "    compute_predictions_logits,\n",
    "    squad_evaluate,\n",
    ")\n",
    "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n",
    "from transformers.trainer_utils import is_main_process\n",
    "\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e987bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--threads'], dest='threads', nargs=None, const=None, default=1, type=<class 'int'>, choices=None, help='multiple threads for converting example to features', metavar=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Required parameters\n",
    "parser.add_argument(\n",
    "    \"--model_type\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"Model type selected in the list: \" + \", \".join(MODEL_TYPES),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name_or_path\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_dir\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"The output directory where the model checkpoints and predictions will be written.\",\n",
    ")\n",
    "\n",
    "# Other parameters\n",
    "parser.add_argument(\n",
    "    \"--data_dir\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    help=\"The input data dir. Should contain the .json files for the task.\"\n",
    "    + \"If no data dir or train/predict files are specified, will run with tensorflow_datasets.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train_file\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    help=\"The input training file. If a data dir is specified, will look for the file there\"\n",
    "    + \"If no data dir or train/predict files are specified, will run with tensorflow_datasets.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--predict_file\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    help=\"The input evaluation file. If a data dir is specified, will look for the file there\"\n",
    "    + \"If no data dir or train/predict files are specified, will run with tensorflow_datasets.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--tokenizer_name\",\n",
    "    default=\"\",\n",
    "    type=str,\n",
    "    help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\",\n",
    "    default=\"\",\n",
    "    type=str,\n",
    "    help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--version_2_with_negative\",\n",
    "    action=\"store_true\",\n",
    "    help=\"If true, the SQuAD examples contain some that do not have an answer.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--null_score_diff_threshold\",\n",
    "    type=float,\n",
    "    default=0.0,\n",
    "    help=\"If null_score - best_non_null is greater than the threshold predict null.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--max_seq_length\",\n",
    "    default=384,\n",
    "    type=int,\n",
    "    help=\"The maximum total input sequence length after WordPiece tokenization. Sequences \"\n",
    "    \"longer than this will be truncated, and sequences shorter than this will be padded.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--doc_stride\",\n",
    "    default=128,\n",
    "    type=int,\n",
    "    help=\"When splitting up a long document into chunks, how much stride to take between chunks.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_query_length\",\n",
    "    default=64,\n",
    "    type=int,\n",
    "    help=\"The maximum number of tokens for the question. Questions longer than this will \"\n",
    "    \"be truncated to this length.\",\n",
    ")\n",
    "parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
    "parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
    "parser.add_argument(\n",
    "    \"--evaluate_during_training\", action=\"store_true\", help=\"Run evaluation during training at each logging step.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--per_gpu_train_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
    "parser.add_argument(\n",
    "    \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n",
    ")\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\n",
    "    \"--gradient_accumulation_steps\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    ")\n",
    "parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "parser.add_argument(\n",
    "    \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_steps\",\n",
    "    default=-1,\n",
    "    type=int,\n",
    "    help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
    ")\n",
    "parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "parser.add_argument(\n",
    "    \"--n_best_size\",\n",
    "    default=20,\n",
    "    type=int,\n",
    "    help=\"The total number of n-best predictions to generate in the nbest_predictions.json output file.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_answer_length\",\n",
    "    default=30,\n",
    "    type=int,\n",
    "    help=\"The maximum length of an answer that can be generated. This is needed because the start \"\n",
    "    \"and end predictions are not conditioned on one another.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--verbose_logging\",\n",
    "    action=\"store_true\",\n",
    "    help=\"If true, all of the warnings related to data processing will be printed. \"\n",
    "    \"A number of warnings are expected for a normal SQuAD evaluation.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lang_id\",\n",
    "    default=0,\n",
    "    type=int,\n",
    "    help=\"language id of input for language-specific xlm models (see tokenization_xlm.PRETRAINED_INIT_CONFIGURATION)\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n",
    "parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n",
    "parser.add_argument(\n",
    "    \"--eval_all_checkpoints\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
    ")\n",
    "parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Whether not to use CUDA when available\")\n",
    "parser.add_argument(\n",
    "    \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n",
    ")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
    "\n",
    "parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"local_rank for distributed training on gpus\")\n",
    "parser.add_argument(\n",
    "    \"--fp16\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--fp16_opt_level\",\n",
    "    type=str,\n",
    "    default=\"O1\",\n",
    "    help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "    \"See details at https://nvidia.github.io/apex/amp.html\",\n",
    ")\n",
    "parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n",
    "parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n",
    "\n",
    "parser.add_argument(\"--threads\", type=int, default=1, help=\"multiple threads for converting example to features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc5a5506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --model_type MODEL_TYPE --model_name_or_path\n",
      "                             MODEL_NAME_OR_PATH --output_dir OUTPUT_DIR\n",
      "                             [--data_dir DATA_DIR] [--train_file TRAIN_FILE]\n",
      "                             [--predict_file PREDICT_FILE]\n",
      "                             [--config_name CONFIG_NAME]\n",
      "                             [--tokenizer_name TOKENIZER_NAME]\n",
      "                             [--cache_dir CACHE_DIR]\n",
      "                             [--version_2_with_negative]\n",
      "                             [--null_score_diff_threshold NULL_SCORE_DIFF_THRESHOLD]\n",
      "                             [--max_seq_length MAX_SEQ_LENGTH]\n",
      "                             [--doc_stride DOC_STRIDE]\n",
      "                             [--max_query_length MAX_QUERY_LENGTH]\n",
      "                             [--do_train] [--do_eval]\n",
      "                             [--evaluate_during_training] [--do_lower_case]\n",
      "                             [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                             [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--max_steps MAX_STEPS]\n",
      "                             [--warmup_steps WARMUP_STEPS]\n",
      "                             [--n_best_size N_BEST_SIZE]\n",
      "                             [--max_answer_length MAX_ANSWER_LENGTH]\n",
      "                             [--verbose_logging] [--lang_id LANG_ID]\n",
      "                             [--logging_steps LOGGING_STEPS]\n",
      "                             [--save_steps SAVE_STEPS]\n",
      "                             [--eval_all_checkpoints] [--no_cuda]\n",
      "                             [--overwrite_output_dir] [--overwrite_cache]\n",
      "                             [--seed SEED] [--local_rank LOCAL_RANK] [--fp16]\n",
      "                             [--fp16_opt_level FP16_OPT_LEVEL]\n",
      "                             [--server_ip SERVER_IP]\n",
      "                             [--server_port SERVER_PORT] [--threads THREADS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: #\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "config = \"--version_2_with_negative \\\n",
    "  --model_type albert \\\n",
    "  --model_name_or_path albert-base-v2 \\\n",
    "  --output_dir ../output/ \\\n",
    "  --do_eval \\\n",
    "  --do_lower_case \\\n",
    "  --train_file ../squad/train-v2.0.json \\\n",
    "  --predict_file ../squad/dev-v2.0.json \\\n",
    "#  --cache_dir ../sample_code \\\n",
    "  --per_gpu_train_batch_size 3 \\\n",
    "  --per_gpu_eval_batch_size 64 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 3.0 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --save_steps 2000 \\\n",
    "  --threads 24 \\\n",
    "  --warmup_steps 814 \\\n",
    "  --gradient_accumulation_steps 4 \\\n",
    "  --do_train\"\n",
    "args = parser.parse_args(config.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.cache_dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8f1250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/29/2021 14:48:20 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "[INFO|file_utils.py:1664] 2021-11-29 14:48:20,221 >> https://huggingface.co/albert-base-v2/resolve/main/config.json not found in cache or force_download set to True, downloading to F:\\My Github\\EECS_595_Proj\\sample_code\\tmpzqej5jua\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_48572/282917446.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m config = AutoConfig.from_pretrained(\n\u001b[0;32m     41\u001b[0m     \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig_name\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig_name\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_dir\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_dir\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m tokenizer = AutoTokenizer.from_pretrained(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\bert\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    525\u001b[0m         \"\"\"\n\u001b[0;32m    526\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_from_auto\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"model_type\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mconfig_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model_type\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\bert\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m                 \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m                 \u001b[0muser_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    555\u001b[0m             )\n\u001b[0;32m    556\u001b[0m             \u001b[1;31m# Load config dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\bert\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1408\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1410\u001b[1;33m             \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1411\u001b[0m         )\n\u001b[0;32m   1412\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\bert\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1664\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{url} not found in cache or force_download set to True, downloading to {temp_file.name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1666\u001b[1;33m             \u001b[0mhttp_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_to_download\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"storing {url} in cache at {cache_path}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\bert\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[0;32m   1521\u001b[0m         \u001b[0minitial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Downloading\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1523\u001b[1;33m         \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_verbosity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1524\u001b[0m     )\n\u001b[0;32m   1525\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\bert\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[0munit_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_printer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mncols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplayed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\bert\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mIProgress\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# #187 #451 #558 #872\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             raise ImportError(\n\u001b[1;32m--> 116\u001b[1;33m                 \u001b[1;34m\"IProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m                 \u001b[1;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                 \"/user_install.html\")\n",
      "\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    args.local_rank,\n",
    "    device,\n",
    "    args.n_gpu,\n",
    "    bool(args.local_rank != -1),\n",
    "    args.fp16,\n",
    ")\n",
    "# Set the verbosity to info of the Transformers logger (on main process only):\n",
    "if is_main_process(args.local_rank):\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "# Set seed\n",
    "set_seed(args)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    # Make sure only the first process in distributed training will download model & vocab\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "args.model_type = args.model_type.lower()\n",
    "config = AutoConfig.from_pretrained(\n",
    "    args.config_name if args.config_name else args.model_name_or_path,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "    do_lower_case=args.do_lower_case,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "    use_fast=False,  # SquadDataset is not compatible with Fast tokenizers which have a smarter overflow handeling\n",
    ")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    # Make sure only the first process in distributed training will download model & vocab\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "# Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum if args.fp16 is set.\n",
    "# Otherwise it'll default to \"promote\" mode, and we'll get fp32 operations. Note that running `--fp16_opt_level=\"O2\"` will\n",
    "# remove the need for this code, but it is still valid.\n",
    "if args.fp16:\n",
    "    try:\n",
    "        import apex\n",
    "\n",
    "        apex.amp.register_half_function(torch, \"einsum\")\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "050834a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n",
    "    if args.local_rank not in [-1, 0] and not evaluate:\n",
    "        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    # Load data features from cache or dataset file\n",
    "    input_dir = args.data_dir if args.data_dir else \".\"\n",
    "    cached_features_file = os.path.join(\n",
    "        input_dir,\n",
    "        \"cached_{}_{}_{}\".format(\n",
    "            \"dev\" if evaluate else \"train\",\n",
    "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
    "            str(args.max_seq_length),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Init features and dataset from cache if it exists\n",
    "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features_and_dataset = torch.load(cached_features_file)\n",
    "        features, dataset, examples = (\n",
    "            features_and_dataset[\"features\"],\n",
    "            features_and_dataset[\"dataset\"],\n",
    "            features_and_dataset[\"examples\"],\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", input_dir)\n",
    "\n",
    "        if not args.data_dir and ((evaluate and not args.predict_file) or (not evaluate and not args.train_file)):\n",
    "            try:\n",
    "                import tensorflow_datasets as tfds\n",
    "            except ImportError:\n",
    "                raise ImportError(\"If not data_dir is specified, tensorflow_datasets needs to be installed.\")\n",
    "\n",
    "            if args.version_2_with_negative:\n",
    "                logger.warning(\"tensorflow_datasets does not handle version 2 of SQuAD.\")\n",
    "\n",
    "            tfds_examples = tfds.load(\"squad\")\n",
    "            examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n",
    "        else:\n",
    "            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n",
    "            if evaluate:\n",
    "                examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file)\n",
    "            else:\n",
    "                examples = processor.get_train_examples(args.data_dir, filename=args.train_file)\n",
    "\n",
    "        features, dataset = squad_convert_examples_to_features(\n",
    "            examples=examples,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=args.max_seq_length,\n",
    "            doc_stride=args.doc_stride,\n",
    "            max_query_length=args.max_query_length,\n",
    "            is_training=not evaluate,\n",
    "            return_dataset=\"pt\",\n",
    "            threads=args.threads,\n",
    "        )\n",
    "\n",
    "        if args.local_rank in [-1, 0]:\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
    "\n",
    "    if args.local_rank == 0 and not evaluate:\n",
    "        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    if output_examples:\n",
    "        return dataset, examples, features\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2ee96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
    "    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
    "\n",
    "    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(dataset)\n",
    "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "\n",
    "    all_results = []\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "            }\n",
    "\n",
    "            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\", \"bart\", \"longformer\"]:\n",
    "                del inputs[\"token_type_ids\"]\n",
    "\n",
    "            feature_indices = batch[3]\n",
    "\n",
    "            # XLNet and XLM use more arguments for their predictions\n",
    "            if args.model_type in [\"xlnet\", \"xlm\"]:\n",
    "                inputs.update({\"cls_index\": batch[4], \"p_mask\": batch[5]})\n",
    "                # for lang_id-sensitive xlm models\n",
    "                if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n",
    "                    inputs.update(\n",
    "                        {\"langs\": (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)}\n",
    "                    )\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        for i, feature_index in enumerate(feature_indices):\n",
    "            eval_feature = features[feature_index.item()]\n",
    "            unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "            output = [to_list(output[i]) for output in outputs.to_tuple()]\n",
    "\n",
    "            # Some models (XLNet, XLM) use 5 arguments for their predictions, while the other \"simpler\"\n",
    "            # models only use two.\n",
    "            if len(output) >= 5:\n",
    "                start_logits = output[0]\n",
    "                start_top_index = output[1]\n",
    "                end_logits = output[2]\n",
    "                end_top_index = output[3]\n",
    "                cls_logits = output[4]\n",
    "\n",
    "                result = SquadResult(\n",
    "                    unique_id,\n",
    "                    start_logits,\n",
    "                    end_logits,\n",
    "                    start_top_index=start_top_index,\n",
    "                    end_top_index=end_top_index,\n",
    "                    cls_logits=cls_logits,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                start_logits, end_logits = output\n",
    "                result = SquadResult(unique_id, start_logits, end_logits)\n",
    "\n",
    "            all_results.append(result)\n",
    "\n",
    "    evalTime = timeit.default_timer() - start_time\n",
    "    logger.info(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n",
    "\n",
    "    # Compute predictions\n",
    "    output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(prefix))\n",
    "    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
    "\n",
    "    if args.version_2_with_negative:\n",
    "        output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds_{}.json\".format(prefix))\n",
    "    else:\n",
    "        output_null_log_odds_file = None\n",
    "\n",
    "    # XLNet and XLM use a more complex post-processing procedure\n",
    "    if args.model_type in [\"xlnet\", \"xlm\"]:\n",
    "        start_n_top = model.config.start_n_top if hasattr(model, \"config\") else model.module.config.start_n_top\n",
    "        end_n_top = model.config.end_n_top if hasattr(model, \"config\") else model.module.config.end_n_top\n",
    "\n",
    "        predictions = compute_predictions_log_probs(\n",
    "            examples,\n",
    "            features,\n",
    "            all_results,\n",
    "            args.n_best_size,\n",
    "            args.max_answer_length,\n",
    "            output_prediction_file,\n",
    "            output_nbest_file,\n",
    "            output_null_log_odds_file,\n",
    "            start_n_top,\n",
    "            end_n_top,\n",
    "            args.version_2_with_negative,\n",
    "            tokenizer,\n",
    "            args.verbose_logging,\n",
    "        )\n",
    "    else:\n",
    "        predictions = compute_predictions_logits(\n",
    "            examples,\n",
    "            features,\n",
    "            all_results,\n",
    "            args.n_best_size,\n",
    "            args.max_answer_length,\n",
    "            args.do_lower_case,\n",
    "            output_prediction_file,\n",
    "            output_nbest_file,\n",
    "            output_null_log_odds_file,\n",
    "            args.verbose_logging,\n",
    "            args.version_2_with_negative,\n",
    "            args.null_score_diff_threshold,\n",
    "            tokenizer,\n",
    "        )\n",
    "\n",
    "    # Compute the F1 and exact scores.\n",
    "    results = squad_evaluate(examples, predictions)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51ffd9",
   "metadata": {},
   "source": [
    "There should be NO need to modify anything abovs else than the config for args class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04c99999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.local_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79f34b",
   "metadata": {},
   "source": [
    "This block generates the original output by albert o squad2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4a7a176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/29/2021 15:01:11 - INFO - __main__ - Loading checkpoints saved during training for evaluation\n",
      "11/29/2021 15:01:11 - INFO - __main__ - Evaluate the following checkpoints: ['../output/']\n",
      "[INFO|configuration_utils.py:581] 2021-11-29 15:01:11,039 >> loading configuration file ../output/config.json\n",
      "[INFO|configuration_utils.py:620] 2021-11-29 15:01:11,040 >> Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1321] 2021-11-29 15:01:11,040 >> loading weights file ../output/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1588] 2021-11-29 15:01:11,115 >> All model checkpoint weights were used when initializing AlbertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1597] 2021-11-29 15:01:11,116 >> All the weights of AlbertForQuestionAnswering were initialized from the model checkpoint at ../output/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForQuestionAnswering for predictions without further training.\n",
      "11/29/2021 15:01:11 - INFO - __main__ - Loading features from cached file .\\cached_dev_albert-base-v2_384\n",
      "11/29/2021 15:01:16 - INFO - __main__ - ***** Running evaluation  *****\n",
      "11/29/2021 15:01:16 - INFO - __main__ -   Num examples = 12272\n",
      "11/29/2021 15:01:16 - INFO - __main__ -   Batch size = 64\n",
      "\n",
      "Evaluating:   0%|                                                                              | 0/192 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:   1%|▎                                                                     | 1/192 [00:00<01:52,  1.69it/s]\u001b[A\n",
      "Evaluating:   1%|▋                                                                     | 2/192 [00:01<01:51,  1.70it/s]\u001b[A\n",
      "Evaluating:   2%|█                                                                     | 3/192 [00:01<01:51,  1.70it/s]\u001b[A\n",
      "Evaluating:   2%|█▍                                                                    | 4/192 [00:02<01:51,  1.69it/s]\u001b[A\n",
      "Evaluating:   3%|█▊                                                                    | 5/192 [00:03<01:55,  1.62it/s]\u001b[A\n",
      "Evaluating:   3%|██▏                                                                   | 6/192 [00:03<01:54,  1.63it/s]\u001b[A\n",
      "Evaluating:   4%|██▌                                                                   | 7/192 [00:04<01:57,  1.57it/s]\u001b[A\n",
      "Evaluating:   4%|██▉                                                                   | 8/192 [00:05<02:00,  1.53it/s]\u001b[A\n",
      "Evaluating:   5%|███▎                                                                  | 9/192 [00:05<02:01,  1.51it/s]\u001b[A\n",
      "Evaluating:   5%|███▌                                                                 | 10/192 [00:06<02:01,  1.49it/s]\u001b[A\n",
      "Evaluating:   6%|███▉                                                                 | 11/192 [00:07<02:01,  1.49it/s]\u001b[A\n",
      "Evaluating:   6%|████▎                                                                | 12/192 [00:07<01:59,  1.51it/s]\u001b[A\n",
      "Evaluating:   7%|████▋                                                                | 13/192 [00:08<01:54,  1.56it/s]\u001b[A\n",
      "Evaluating:   7%|█████                                                                | 14/192 [00:08<01:51,  1.60it/s]\u001b[A\n",
      "Evaluating:   8%|█████▍                                                               | 15/192 [00:09<01:48,  1.63it/s]\u001b[A\n",
      "Evaluating:   8%|█████▊                                                               | 16/192 [00:10<01:46,  1.65it/s]\u001b[A\n",
      "Evaluating:   9%|██████                                                               | 17/192 [00:10<01:45,  1.66it/s]\u001b[A\n",
      "Evaluating:   9%|██████▍                                                              | 18/192 [00:11<01:44,  1.67it/s]\u001b[A\n",
      "Evaluating:  10%|██████▊                                                              | 19/192 [00:11<01:43,  1.68it/s]\u001b[A\n",
      "Evaluating:  10%|███████▏                                                             | 20/192 [00:12<01:42,  1.68it/s]\u001b[A\n",
      "Evaluating:  11%|███████▌                                                             | 21/192 [00:13<01:41,  1.69it/s]\u001b[A\n",
      "Evaluating:  11%|███████▉                                                             | 22/192 [00:13<01:40,  1.69it/s]\u001b[A\n",
      "Evaluating:  12%|████████▎                                                            | 23/192 [00:14<01:39,  1.70it/s]\u001b[A\n",
      "Evaluating:  12%|████████▋                                                            | 24/192 [00:14<01:39,  1.70it/s]\u001b[A\n",
      "Evaluating:  13%|████████▉                                                            | 25/192 [00:15<01:38,  1.70it/s]\u001b[A\n",
      "Evaluating:  14%|█████████▎                                                           | 26/192 [00:15<01:37,  1.70it/s]\u001b[A\n",
      "Evaluating:  14%|█████████▋                                                           | 27/192 [00:16<01:37,  1.70it/s]\u001b[A\n",
      "Evaluating:  15%|██████████                                                           | 28/192 [00:17<01:36,  1.70it/s]\u001b[A\n",
      "Evaluating:  15%|██████████▍                                                          | 29/192 [00:17<01:35,  1.70it/s]\u001b[A\n",
      "Evaluating:  16%|██████████▊                                                          | 30/192 [00:18<01:36,  1.68it/s]\u001b[A\n",
      "Evaluating:  16%|███████████▏                                                         | 31/192 [00:18<01:39,  1.63it/s]\u001b[A\n",
      "Evaluating:  17%|███████████▌                                                         | 32/192 [00:19<01:37,  1.65it/s]\u001b[A\n",
      "Evaluating:  17%|███████████▊                                                         | 33/192 [00:20<01:35,  1.67it/s]\u001b[A\n",
      "Evaluating:  18%|████████████▏                                                        | 34/192 [00:20<01:36,  1.64it/s]\u001b[A\n",
      "Evaluating:  18%|████████████▌                                                        | 35/192 [00:21<01:38,  1.60it/s]\u001b[A\n",
      "Evaluating:  19%|████████████▉                                                        | 36/192 [00:22<01:39,  1.57it/s]\u001b[A\n",
      "Evaluating:  19%|█████████████▎                                                       | 37/192 [00:22<01:37,  1.59it/s]\u001b[A\n",
      "Evaluating:  20%|█████████████▋                                                       | 38/192 [00:23<01:34,  1.62it/s]\u001b[A\n",
      "Evaluating:  20%|██████████████                                                       | 39/192 [00:23<01:32,  1.65it/s]\u001b[A\n",
      "Evaluating:  21%|██████████████▍                                                      | 40/192 [00:24<01:31,  1.66it/s]\u001b[A\n",
      "Evaluating:  21%|██████████████▋                                                      | 41/192 [00:25<01:30,  1.68it/s]\u001b[A\n",
      "Evaluating:  22%|███████████████                                                      | 42/192 [00:25<01:31,  1.64it/s]\u001b[A\n",
      "Evaluating:  22%|███████████████▍                                                     | 43/192 [00:26<01:31,  1.62it/s]\u001b[A\n",
      "Evaluating:  23%|███████████████▊                                                     | 44/192 [00:27<01:33,  1.58it/s]\u001b[A\n",
      "Evaluating:  23%|████████████████▏                                                    | 45/192 [00:27<01:34,  1.55it/s]\u001b[A\n",
      "Evaluating:  24%|████████████████▌                                                    | 46/192 [00:28<01:33,  1.56it/s]\u001b[A\n",
      "Evaluating:  24%|████████████████▉                                                    | 47/192 [00:28<01:34,  1.53it/s]\u001b[A\n",
      "Evaluating:  25%|█████████████████▎                                                   | 48/192 [00:29<01:35,  1.51it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  26%|█████████████████▌                                                   | 49/192 [00:30<01:34,  1.51it/s]\u001b[A\n",
      "Evaluating:  26%|█████████████████▉                                                   | 50/192 [00:30<01:33,  1.51it/s]\u001b[A\n",
      "Evaluating:  27%|██████████████████▎                                                  | 51/192 [00:31<01:30,  1.56it/s]\u001b[A\n",
      "Evaluating:  27%|██████████████████▋                                                  | 52/192 [00:32<01:27,  1.59it/s]\u001b[A\n",
      "Evaluating:  28%|███████████████████                                                  | 53/192 [00:32<01:25,  1.62it/s]\u001b[A\n",
      "Evaluating:  28%|███████████████████▍                                                 | 54/192 [00:33<01:23,  1.64it/s]\u001b[A\n",
      "Evaluating:  29%|███████████████████▊                                                 | 55/192 [00:33<01:22,  1.66it/s]\u001b[A\n",
      "Evaluating:  29%|████████████████████▏                                                | 56/192 [00:34<01:21,  1.67it/s]\u001b[A\n",
      "Evaluating:  30%|████████████████████▍                                                | 57/192 [00:35<01:20,  1.68it/s]\u001b[A\n",
      "Evaluating:  30%|████████████████████▊                                                | 58/192 [00:35<01:19,  1.69it/s]\u001b[A\n",
      "Evaluating:  31%|█████████████████████▏                                               | 59/192 [00:36<01:18,  1.69it/s]\u001b[A\n",
      "Evaluating:  31%|█████████████████████▌                                               | 60/192 [00:36<01:17,  1.69it/s]\u001b[A\n",
      "Evaluating:  32%|█████████████████████▉                                               | 61/192 [00:37<01:17,  1.69it/s]\u001b[A\n",
      "Evaluating:  32%|██████████████████████▎                                              | 62/192 [00:38<01:16,  1.70it/s]\u001b[A\n",
      "Evaluating:  33%|██████████████████████▋                                              | 63/192 [00:38<01:16,  1.70it/s]\u001b[A\n",
      "Evaluating:  33%|███████████████████████                                              | 64/192 [00:39<01:15,  1.70it/s]\u001b[A\n",
      "Evaluating:  34%|███████████████████████▎                                             | 65/192 [00:39<01:14,  1.70it/s]\u001b[A\n",
      "Evaluating:  34%|███████████████████████▋                                             | 66/192 [00:40<01:14,  1.70it/s]\u001b[A\n",
      "Evaluating:  35%|████████████████████████                                             | 67/192 [00:41<01:13,  1.70it/s]\u001b[A\n",
      "Evaluating:  35%|████████████████████████▍                                            | 68/192 [00:41<01:13,  1.70it/s]\u001b[A\n",
      "Evaluating:  36%|████████████████████████▊                                            | 69/192 [00:42<01:12,  1.70it/s]\u001b[A\n",
      "Evaluating:  36%|█████████████████████████▏                                           | 70/192 [00:42<01:11,  1.70it/s]\u001b[A\n",
      "Evaluating:  37%|█████████████████████████▌                                           | 71/192 [00:43<01:11,  1.70it/s]\u001b[A\n",
      "Evaluating:  38%|█████████████████████████▉                                           | 72/192 [00:43<01:10,  1.70it/s]\u001b[A\n",
      "Evaluating:  38%|██████████████████████████▏                                          | 73/192 [00:44<01:10,  1.70it/s]\u001b[A\n",
      "Evaluating:  39%|██████████████████████████▌                                          | 74/192 [00:45<01:09,  1.70it/s]\u001b[A\n",
      "Evaluating:  39%|██████████████████████████▉                                          | 75/192 [00:45<01:08,  1.70it/s]\u001b[A\n",
      "Evaluating:  40%|███████████████████████████▎                                         | 76/192 [00:46<01:08,  1.70it/s]\u001b[A\n",
      "Evaluating:  40%|███████████████████████████▋                                         | 77/192 [00:46<01:07,  1.70it/s]\u001b[A\n",
      "Evaluating:  41%|████████████████████████████                                         | 78/192 [00:47<01:07,  1.70it/s]\u001b[A\n",
      "Evaluating:  41%|████████████████████████████▍                                        | 79/192 [00:48<01:06,  1.70it/s]\u001b[A\n",
      "Evaluating:  42%|████████████████████████████▊                                        | 80/192 [00:48<01:05,  1.70it/s]\u001b[A\n",
      "Evaluating:  42%|█████████████████████████████                                        | 81/192 [00:49<01:05,  1.70it/s]\u001b[A\n",
      "Evaluating:  43%|█████████████████████████████▍                                       | 82/192 [00:49<01:04,  1.70it/s]\u001b[A\n",
      "Evaluating:  43%|█████████████████████████████▊                                       | 83/192 [00:50<01:05,  1.65it/s]\u001b[A\n",
      "Evaluating:  44%|██████████████████████████████▏                                      | 84/192 [00:51<01:07,  1.59it/s]\u001b[A\n",
      "Evaluating:  44%|██████████████████████████████▌                                      | 85/192 [00:51<01:08,  1.55it/s]\u001b[A\n",
      "Evaluating:  45%|██████████████████████████████▉                                      | 86/192 [00:52<01:06,  1.58it/s]\u001b[A\n",
      "Evaluating:  45%|███████████████████████████████▎                                     | 87/192 [00:53<01:04,  1.62it/s]\u001b[A\n",
      "Evaluating:  46%|███████████████████████████████▋                                     | 88/192 [00:53<01:03,  1.64it/s]\u001b[A\n",
      "Evaluating:  46%|███████████████████████████████▉                                     | 89/192 [00:54<01:03,  1.63it/s]\u001b[A\n",
      "Evaluating:  47%|████████████████████████████████▎                                    | 90/192 [00:54<01:01,  1.65it/s]\u001b[A\n",
      "Evaluating:  47%|████████████████████████████████▋                                    | 91/192 [00:55<01:00,  1.66it/s]\u001b[A\n",
      "Evaluating:  48%|█████████████████████████████████                                    | 92/192 [00:56<00:59,  1.67it/s]\u001b[A\n",
      "Evaluating:  48%|█████████████████████████████████▍                                   | 93/192 [00:56<00:58,  1.68it/s]\u001b[A\n",
      "Evaluating:  49%|█████████████████████████████████▊                                   | 94/192 [00:57<00:58,  1.68it/s]\u001b[A\n",
      "Evaluating:  49%|██████████████████████████████████▏                                  | 95/192 [00:57<00:57,  1.69it/s]\u001b[A\n",
      "Evaluating:  50%|██████████████████████████████████▌                                  | 96/192 [00:58<00:56,  1.69it/s]\u001b[A\n",
      "Evaluating:  51%|██████████████████████████████████▊                                  | 97/192 [00:58<00:56,  1.69it/s]\u001b[A\n",
      "Evaluating:  51%|███████████████████████████████████▏                                 | 98/192 [00:59<00:55,  1.69it/s]\u001b[A\n",
      "Evaluating:  52%|███████████████████████████████████▌                                 | 99/192 [01:00<00:55,  1.69it/s]\u001b[A\n",
      "Evaluating:  52%|███████████████████████████████████▍                                | 100/192 [01:00<00:54,  1.69it/s]\u001b[A\n",
      "Evaluating:  53%|███████████████████████████████████▊                                | 101/192 [01:01<00:53,  1.69it/s]\u001b[A\n",
      "Evaluating:  53%|████████████████████████████████████▏                               | 102/192 [01:01<00:53,  1.69it/s]\u001b[A\n",
      "Evaluating:  54%|████████████████████████████████████▍                               | 103/192 [01:02<00:52,  1.69it/s]\u001b[A\n",
      "Evaluating:  54%|████████████████████████████████████▊                               | 104/192 [01:03<00:52,  1.68it/s]\u001b[A\n",
      "Evaluating:  55%|█████████████████████████████████████▏                              | 105/192 [01:03<00:51,  1.68it/s]\u001b[A\n",
      "Evaluating:  55%|█████████████████████████████████████▌                              | 106/192 [01:04<00:51,  1.68it/s]\u001b[A\n",
      "Evaluating:  56%|█████████████████████████████████████▉                              | 107/192 [01:04<00:50,  1.68it/s]\u001b[A\n",
      "Evaluating:  56%|██████████████████████████████████████▎                             | 108/192 [01:05<00:50,  1.67it/s]\u001b[A\n",
      "Evaluating:  57%|██████████████████████████████████████▌                             | 109/192 [01:06<00:49,  1.68it/s]\u001b[A\n",
      "Evaluating:  57%|██████████████████████████████████████▉                             | 110/192 [01:06<00:48,  1.68it/s]\u001b[A\n",
      "Evaluating:  58%|███████████████████████████████████████▎                            | 111/192 [01:07<00:48,  1.68it/s]\u001b[A\n",
      "Evaluating:  58%|███████████████████████████████████████▋                            | 112/192 [01:07<00:47,  1.68it/s]\u001b[A\n",
      "Evaluating:  59%|████████████████████████████████████████                            | 113/192 [01:08<00:46,  1.69it/s]\u001b[A\n",
      "Evaluating:  59%|████████████████████████████████████████▍                           | 114/192 [01:09<00:46,  1.68it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  60%|████████████████████████████████████████▋                           | 115/192 [01:09<00:45,  1.68it/s]\u001b[A\n",
      "Evaluating:  60%|█████████████████████████████████████████                           | 116/192 [01:10<00:45,  1.69it/s]\u001b[A\n",
      "Evaluating:  61%|█████████████████████████████████████████▍                          | 117/192 [01:10<00:44,  1.69it/s]\u001b[A\n",
      "Evaluating:  61%|█████████████████████████████████████████▊                          | 118/192 [01:11<00:43,  1.69it/s]\u001b[A\n",
      "Evaluating:  62%|██████████████████████████████████████████▏                         | 119/192 [01:12<00:43,  1.69it/s]\u001b[A\n",
      "Evaluating:  62%|██████████████████████████████████████████▌                         | 120/192 [01:12<00:42,  1.69it/s]\u001b[A\n",
      "Evaluating:  63%|██████████████████████████████████████████▊                         | 121/192 [01:13<00:42,  1.69it/s]\u001b[A\n",
      "Evaluating:  64%|███████████████████████████████████████████▏                        | 122/192 [01:13<00:41,  1.69it/s]\u001b[A\n",
      "Evaluating:  64%|███████████████████████████████████████████▌                        | 123/192 [01:14<00:40,  1.69it/s]\u001b[A\n",
      "Evaluating:  65%|███████████████████████████████████████████▉                        | 124/192 [01:15<00:40,  1.69it/s]\u001b[A\n",
      "Evaluating:  65%|████████████████████████████████████████████▎                       | 125/192 [01:15<00:39,  1.69it/s]\u001b[A\n",
      "Evaluating:  66%|████████████████████████████████████████████▋                       | 126/192 [01:16<00:39,  1.69it/s]\u001b[A\n",
      "Evaluating:  66%|████████████████████████████████████████████▉                       | 127/192 [01:16<00:38,  1.69it/s]\u001b[A\n",
      "Evaluating:  67%|█████████████████████████████████████████████▎                      | 128/192 [01:17<00:38,  1.68it/s]\u001b[A\n",
      "Evaluating:  67%|█████████████████████████████████████████████▋                      | 129/192 [01:17<00:37,  1.68it/s]\u001b[A\n",
      "Evaluating:  68%|██████████████████████████████████████████████                      | 130/192 [01:18<00:36,  1.68it/s]\u001b[A\n",
      "Evaluating:  68%|██████████████████████████████████████████████▍                     | 131/192 [01:19<00:36,  1.69it/s]\u001b[A\n",
      "Evaluating:  69%|██████████████████████████████████████████████▊                     | 132/192 [01:19<00:35,  1.69it/s]\u001b[A\n",
      "Evaluating:  69%|███████████████████████████████████████████████                     | 133/192 [01:20<00:35,  1.67it/s]\u001b[A\n",
      "Evaluating:  70%|███████████████████████████████████████████████▍                    | 134/192 [01:20<00:34,  1.67it/s]\u001b[A\n",
      "Evaluating:  70%|███████████████████████████████████████████████▊                    | 135/192 [01:21<00:33,  1.68it/s]\u001b[A\n",
      "Evaluating:  71%|████████████████████████████████████████████████▏                   | 136/192 [01:22<00:33,  1.67it/s]\u001b[A\n",
      "Evaluating:  71%|████████████████████████████████████████████████▌                   | 137/192 [01:22<00:32,  1.67it/s]\u001b[A\n",
      "Evaluating:  72%|████████████████████████████████████████████████▉                   | 138/192 [01:23<00:33,  1.61it/s]\u001b[A\n",
      "Evaluating:  72%|█████████████████████████████████████████████████▏                  | 139/192 [01:24<00:32,  1.64it/s]\u001b[A\n",
      "Evaluating:  73%|█████████████████████████████████████████████████▌                  | 140/192 [01:24<00:31,  1.65it/s]\u001b[A\n",
      "Evaluating:  73%|█████████████████████████████████████████████████▉                  | 141/192 [01:25<00:30,  1.67it/s]\u001b[A\n",
      "Evaluating:  74%|██████████████████████████████████████████████████▎                 | 142/192 [01:25<00:31,  1.60it/s]\u001b[A\n",
      "Evaluating:  74%|██████████████████████████████████████████████████▋                 | 143/192 [01:26<00:30,  1.62it/s]\u001b[A\n",
      "Evaluating:  75%|███████████████████████████████████████████████████                 | 144/192 [01:27<00:29,  1.63it/s]\u001b[A\n",
      "Evaluating:  76%|███████████████████████████████████████████████████▎                | 145/192 [01:27<00:28,  1.64it/s]\u001b[A\n",
      "Evaluating:  76%|███████████████████████████████████████████████████▋                | 146/192 [01:28<00:27,  1.65it/s]\u001b[A\n",
      "Evaluating:  77%|████████████████████████████████████████████████████                | 147/192 [01:28<00:27,  1.66it/s]\u001b[A\n",
      "Evaluating:  77%|████████████████████████████████████████████████████▍               | 148/192 [01:29<00:26,  1.65it/s]\u001b[A\n",
      "Evaluating:  78%|████████████████████████████████████████████████████▊               | 149/192 [01:30<00:26,  1.64it/s]\u001b[A\n",
      "Evaluating:  78%|█████████████████████████████████████████████████████▏              | 150/192 [01:30<00:25,  1.65it/s]\u001b[A\n",
      "Evaluating:  79%|█████████████████████████████████████████████████████▍              | 151/192 [01:31<00:24,  1.66it/s]\u001b[A\n",
      "Evaluating:  79%|█████████████████████████████████████████████████████▊              | 152/192 [01:31<00:23,  1.67it/s]\u001b[A\n",
      "Evaluating:  80%|██████████████████████████████████████████████████████▏             | 153/192 [01:32<00:23,  1.68it/s]\u001b[A\n",
      "Evaluating:  80%|██████████████████████████████████████████████████████▌             | 154/192 [01:33<00:22,  1.68it/s]\u001b[A\n",
      "Evaluating:  81%|██████████████████████████████████████████████████████▉             | 155/192 [01:33<00:21,  1.68it/s]\u001b[A\n",
      "Evaluating:  81%|███████████████████████████████████████████████████████▎            | 156/192 [01:34<00:21,  1.69it/s]\u001b[A\n",
      "Evaluating:  82%|███████████████████████████████████████████████████████▌            | 157/192 [01:34<00:20,  1.69it/s]\u001b[A\n",
      "Evaluating:  82%|███████████████████████████████████████████████████████▉            | 158/192 [01:35<00:20,  1.69it/s]\u001b[A\n",
      "Evaluating:  83%|████████████████████████████████████████████████████████▎           | 159/192 [01:36<00:19,  1.69it/s]\u001b[A\n",
      "Evaluating:  83%|████████████████████████████████████████████████████████▋           | 160/192 [01:36<00:18,  1.69it/s]\u001b[A\n",
      "Evaluating:  84%|█████████████████████████████████████████████████████████           | 161/192 [01:37<00:18,  1.69it/s]\u001b[A\n",
      "Evaluating:  84%|█████████████████████████████████████████████████████████▍          | 162/192 [01:37<00:17,  1.69it/s]\u001b[A\n",
      "Evaluating:  85%|█████████████████████████████████████████████████████████▋          | 163/192 [01:38<00:17,  1.69it/s]\u001b[A\n",
      "Evaluating:  85%|██████████████████████████████████████████████████████████          | 164/192 [01:38<00:16,  1.69it/s]\u001b[A\n",
      "Evaluating:  86%|██████████████████████████████████████████████████████████▍         | 165/192 [01:39<00:15,  1.69it/s]\u001b[A\n",
      "Evaluating:  86%|██████████████████████████████████████████████████████████▊         | 166/192 [01:40<00:15,  1.69it/s]\u001b[A\n",
      "Evaluating:  87%|███████████████████████████████████████████████████████████▏        | 167/192 [01:40<00:14,  1.69it/s]\u001b[A\n",
      "Evaluating:  88%|███████████████████████████████████████████████████████████▌        | 168/192 [01:41<00:14,  1.69it/s]\u001b[A\n",
      "Evaluating:  88%|███████████████████████████████████████████████████████████▊        | 169/192 [01:41<00:13,  1.69it/s]\u001b[A\n",
      "Evaluating:  89%|████████████████████████████████████████████████████████████▏       | 170/192 [01:42<00:12,  1.69it/s]\u001b[A\n",
      "Evaluating:  89%|████████████████████████████████████████████████████████████▌       | 171/192 [01:43<00:12,  1.69it/s]\u001b[A\n",
      "Evaluating:  90%|████████████████████████████████████████████████████████████▉       | 172/192 [01:43<00:11,  1.69it/s]\u001b[A\n",
      "Evaluating:  90%|█████████████████████████████████████████████████████████████▎      | 173/192 [01:44<00:11,  1.69it/s]\u001b[A\n",
      "Evaluating:  91%|█████████████████████████████████████████████████████████████▋      | 174/192 [01:44<00:10,  1.69it/s]\u001b[A\n",
      "Evaluating:  91%|█████████████████████████████████████████████████████████████▉      | 175/192 [01:45<00:10,  1.69it/s]\u001b[A\n",
      "Evaluating:  92%|██████████████████████████████████████████████████████████████▎     | 176/192 [01:46<00:09,  1.69it/s]\u001b[A\n",
      "Evaluating:  92%|██████████████████████████████████████████████████████████████▋     | 177/192 [01:46<00:08,  1.69it/s]\u001b[A\n",
      "Evaluating:  93%|███████████████████████████████████████████████████████████████     | 178/192 [01:47<00:08,  1.69it/s]\u001b[A\n",
      "Evaluating:  93%|███████████████████████████████████████████████████████████████▍    | 179/192 [01:47<00:07,  1.69it/s]\u001b[A\n",
      "Evaluating:  94%|███████████████████████████████████████████████████████████████▊    | 180/192 [01:48<00:07,  1.69it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  94%|████████████████████████████████████████████████████████████████    | 181/192 [01:49<00:06,  1.69it/s]\u001b[A\n",
      "Evaluating:  95%|████████████████████████████████████████████████████████████████▍   | 182/192 [01:49<00:05,  1.69it/s]\u001b[A\n",
      "Evaluating:  95%|████████████████████████████████████████████████████████████████▊   | 183/192 [01:50<00:05,  1.69it/s]\u001b[A\n",
      "Evaluating:  96%|█████████████████████████████████████████████████████████████████▏  | 184/192 [01:50<00:04,  1.69it/s]\u001b[A\n",
      "Evaluating:  96%|█████████████████████████████████████████████████████████████████▌  | 185/192 [01:51<00:04,  1.69it/s]\u001b[A\n",
      "Evaluating:  97%|█████████████████████████████████████████████████████████████████▉  | 186/192 [01:52<00:03,  1.69it/s]\u001b[A\n",
      "Evaluating:  97%|██████████████████████████████████████████████████████████████████▏ | 187/192 [01:52<00:02,  1.69it/s]\u001b[A\n",
      "Evaluating:  98%|██████████████████████████████████████████████████████████████████▌ | 188/192 [01:53<00:02,  1.69it/s]\u001b[A\n",
      "Evaluating:  98%|██████████████████████████████████████████████████████████████████▉ | 189/192 [01:53<00:01,  1.69it/s]\u001b[A\n",
      "Evaluating:  99%|███████████████████████████████████████████████████████████████████▎| 190/192 [01:54<00:01,  1.69it/s]\u001b[A\n",
      "Evaluating:  99%|███████████████████████████████████████████████████████████████████▋| 191/192 [01:54<00:00,  1.69it/s]\u001b[A\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 192/192 [01:55<00:00,  1.66it/s]\u001b[A\n",
      "11/29/2021 15:03:12 - INFO - __main__ -   Evaluation done in total 115.401166 secs (0.009404 sec per example)\n",
      "[INFO|squad_metrics.py:401] 2021-11-29 15:03:12,019 >> Writing predictions to: ../output/predictions_.json\n",
      "[INFO|squad_metrics.py:403] 2021-11-29 15:03:12,019 >> Writing nbest to: ../output/nbest_predictions_.json\n",
      "[INFO|squad_metrics.py:405] 2021-11-29 15:03:12,020 >> Writing null_log_odds to: ../output/null_odds_.json\n",
      "11/29/2021 15:03:43 - INFO - __main__ - Results: {'exact': 78.20264465594205, 'f1': 81.51628070699975, 'total': 11873, 'HasAns_exact': 74.66261808367072, 'HasAns_f1': 81.29939285327433, 'HasAns_total': 5928, 'NoAns_exact': 81.73254835996636, 'NoAns_f1': 81.73254835996636, 'NoAns_total': 5945, 'best_exact': 78.20264465594205, 'best_exact_thresh': 0.0, 'best_f1': 81.51628070699964, 'best_f1_thresh': 0.0}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "logger.info(\"Loading checkpoints saved during training for evaluation\")\n",
    "checkpoints = [args.output_dir]\n",
    "if args.eval_all_checkpoints:\n",
    "    checkpoints = list(\n",
    "        os.path.dirname(c)\n",
    "        for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "    )\n",
    "logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "for checkpoint in checkpoints:\n",
    "    # Reload the model\n",
    "    global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)  # , force_download=True)\n",
    "    model.to(args.device)\n",
    "\n",
    "    # Evaluate\n",
    "    result = evaluate(args, model, tokenizer, prefix=global_step)\n",
    "\n",
    "    result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n",
    "    results.update(result)\n",
    "logger.info(\"Results: {}\".format(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68788ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 78.20264465594205,\n",
       " 'f1': 81.51628070699975,\n",
       " 'total': 11873,\n",
       " 'HasAns_exact': 74.66261808367072,\n",
       " 'HasAns_f1': 81.29939285327433,\n",
       " 'HasAns_total': 5928,\n",
       " 'NoAns_exact': 81.73254835996636,\n",
       " 'NoAns_f1': 81.73254835996636,\n",
       " 'NoAns_total': 5945,\n",
       " 'best_exact': 78.20264465594205,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 81.51628070699964,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574023b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_compute_predictions_logits(\n",
    "    all_examples,\n",
    "    all_features,\n",
    "    all_results,\n",
    "    n_best_size,\n",
    "    max_answer_length,\n",
    "    do_lower_case,\n",
    "    output_prediction_file,\n",
    "    output_nbest_file,\n",
    "    output_null_log_odds_file,\n",
    "    verbose_logging,\n",
    "    version_2_with_negative,\n",
    "    null_score_diff_threshold,\n",
    "    tokenizer,\n",
    "):\n",
    "    \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n",
    "    if output_prediction_file:\n",
    "        logger.info(f\"Writing predictions to: {output_prediction_file}\")\n",
    "    if output_nbest_file:\n",
    "        logger.info(f\"Writing nbest to: {output_nbest_file}\")\n",
    "    if output_null_log_odds_file and version_2_with_negative:\n",
    "        logger.info(f\"Writing null_log_odds to: {output_null_log_odds_file}\")\n",
    "\n",
    "    example_index_to_features = collections.defaultdict(list)\n",
    "    for feature in all_features:\n",
    "        example_index_to_features[feature.example_index].append(feature)\n",
    "\n",
    "    unique_id_to_result = {}\n",
    "    for result in all_results:\n",
    "        unique_id_to_result[result.unique_id] = result\n",
    "\n",
    "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"PrelimPrediction\", [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"]\n",
    "    )\n",
    "\n",
    "    all_predictions = collections.OrderedDict()\n",
    "    all_nbest_json = collections.OrderedDict()\n",
    "    scores_diff_json = collections.OrderedDict()\n",
    "    all_index_predictions = collections.OrderedDict()\n",
    "\n",
    "    for (example_index, example) in enumerate(all_examples):\n",
    "        features = example_index_to_features[example_index]\n",
    "\n",
    "        prelim_predictions = []\n",
    "        # keep track of the minimum score of null start+end of position 0\n",
    "        score_null = 1000000  # large and positive\n",
    "        min_null_feature_index = 0  # the paragraph slice with min null score\n",
    "        null_start_logit = 0  # the start logit at the slice with min null score\n",
    "        null_end_logit = 0  # the end logit at the slice with min null score\n",
    "        for (feature_index, feature) in enumerate(features):\n",
    "            result = unique_id_to_result[feature.unique_id]\n",
    "            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
    "            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
    "            # if we could have irrelevant answers, get the min score of irrelevant\n",
    "            if version_2_with_negative:\n",
    "                feature_null_score = result.start_logits[0] + result.end_logits[0]\n",
    "                if feature_null_score < score_null:\n",
    "                    score_null = feature_null_score\n",
    "                    min_null_feature_index = feature_index\n",
    "                    null_start_logit = result.start_logits[0]\n",
    "                    null_end_logit = result.end_logits[0]\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # We could hypothetically create invalid predictions, e.g., predict\n",
    "                    # that the start of the span is in the question. We throw out all\n",
    "                    # invalid predictions.\n",
    "                    if start_index >= len(feature.tokens):\n",
    "                        continue\n",
    "                    if end_index >= len(feature.tokens):\n",
    "                        continue\n",
    "                    if start_index not in feature.token_to_orig_map:\n",
    "                        continue\n",
    "                    if end_index not in feature.token_to_orig_map:\n",
    "                        continue\n",
    "                    if not feature.token_is_max_context.get(start_index, False):\n",
    "                        continue\n",
    "                    if end_index < start_index:\n",
    "                        continue\n",
    "                    length = end_index - start_index + 1\n",
    "                    if length > max_answer_length:\n",
    "                        continue\n",
    "                    prelim_predictions.append(\n",
    "                        _PrelimPrediction(\n",
    "                            feature_index=feature_index,\n",
    "                            start_index=start_index,\n",
    "                            end_index=end_index,\n",
    "                            start_logit=result.start_logits[start_index],\n",
    "                            end_logit=result.end_logits[end_index],\n",
    "                        )\n",
    "                    )\n",
    "        if version_2_with_negative:\n",
    "            prelim_predictions.append(\n",
    "                _PrelimPrediction(\n",
    "                    feature_index=min_null_feature_index,\n",
    "                    start_index=0,\n",
    "                    end_index=0,\n",
    "                    start_logit=null_start_logit,\n",
    "                    end_logit=null_end_logit,\n",
    "                )\n",
    "            )\n",
    "        prelim_predictions = sorted(prelim_predictions, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\n",
    "\n",
    "        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"]\n",
    "        )\n",
    "\n",
    "        seen_predictions = {}\n",
    "        nbest = []\n",
    "        nbest_index = []\n",
    "        for pred in prelim_predictions:\n",
    "            if len(nbest) >= n_best_size:\n",
    "                break\n",
    "            feature = features[pred.feature_index]\n",
    "            if pred.start_index > 0:  # this is a non-null prediction\n",
    "                tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n",
    "                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
    "                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
    "                orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n",
    "\n",
    "                tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n",
    "\n",
    "                # tok_text = \" \".join(tok_tokens)\n",
    "                #\n",
    "                # # De-tokenize WordPieces that have been split off.\n",
    "                # tok_text = tok_text.replace(\" ##\", \"\")\n",
    "                # tok_text = tok_text.replace(\"##\", \"\")\n",
    "\n",
    "                # Clean whitespace\n",
    "                tok_text = tok_text.strip()\n",
    "                tok_text = \" \".join(tok_text.split())\n",
    "                orig_text = \" \".join(orig_tokens)\n",
    "\n",
    "                final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)\n",
    "                if final_text in seen_predictions:\n",
    "                    continue\n",
    "\n",
    "                seen_predictions[final_text] = True\n",
    "            else:\n",
    "                final_text = \"\"\n",
    "                seen_predictions[final_text] = True\n",
    "\n",
    "            nbest_index.append((pred.start_index,pred.end_index))\n",
    "            nbest.append(_NbestPrediction(text=final_text, start_logit=pred.start_logit, end_logit=pred.end_logit))\n",
    "        # if we didn't include the empty option in the n-best, include it\n",
    "        if version_2_with_negative:\n",
    "            if \"\" not in seen_predictions:\n",
    "                nbest.append(_NbestPrediction(text=\"\", start_logit=null_start_logit, end_logit=null_end_logit))\n",
    "                nbest_index.append((-1, -1))\n",
    "\n",
    "            # In very rare edge cases we could only have single null prediction.\n",
    "            # So we just create a nonce prediction in this case to avoid failure.\n",
    "            if len(nbest) == 1:\n",
    "                nbest.insert(0, _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
    "                nbest_index.append((-1, -1))\n",
    "\n",
    "        # In very rare edge cases we could have no valid predictions. So we\n",
    "        # just create a nonce prediction in this case to avoid failure.\n",
    "        if not nbest:\n",
    "            nbest.append(_NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
    "            nbest_index.append((-1, -1))\n",
    "\n",
    "        assert len(nbest) >= 1, \"No valid predictions\"\n",
    "\n",
    "        total_scores = []\n",
    "        best_non_null_entry = None\n",
    "        best_index = None\n",
    "        for (i, entry) in enumerate(nbest):\n",
    "            total_scores.append(entry.start_logit + entry.end_logit)\n",
    "            if not best_non_null_entry:\n",
    "                if entry.text:\n",
    "                    best_non_null_entry = entry\n",
    "                    best_index = nbest_index[i]\n",
    "\n",
    "        probs = _compute_softmax(total_scores)\n",
    "\n",
    "        nbest_json = []\n",
    "        for (i, entry) in enumerate(nbest):\n",
    "            output = collections.OrderedDict()\n",
    "            output[\"text\"] = entry.text\n",
    "            output[\"probability\"] = probs[i]\n",
    "            output[\"start_logit\"] = entry.start_logit\n",
    "            output[\"end_logit\"] = entry.end_logit\n",
    "            nbest_json.append(output)\n",
    "\n",
    "        assert len(nbest_json) >= 1, \"No valid predictions\"\n",
    "\n",
    "        if not version_2_with_negative:\n",
    "            all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n",
    "\n",
    "        else:\n",
    "            # predict \"\" iff the null score - the score of best non-null > threshold\n",
    "            score_diff = score_null - best_non_null_entry.start_logit - (best_non_null_entry.end_logit)\n",
    "            scores_diff_json[example.qas_id] = score_diff\n",
    "            if score_diff > null_score_diff_threshold:\n",
    "                all_predictions[example.qas_id] = \"\"\n",
    "            else:\n",
    "                all_predictions[example.qas_id] = best_non_null_entry.text\n",
    "        all_index_predictions[example.qas_id] = best_index\n",
    "        all_nbest_json[example.qas_id] = nbest_json\n",
    "\n",
    "    if output_prediction_file:\n",
    "        with open(output_prediction_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
    "\n",
    "    if output_nbest_file:\n",
    "        with open(output_nbest_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
    "\n",
    "    if output_null_log_odds_file and version_2_with_negative:\n",
    "        with open(output_null_log_odds_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
    "\n",
    "    return all_predictions, all_index_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6ce7e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/29/2021 16:02:06 - INFO - __main__ - Loading checkpoints saved during training for evaluation\n",
      "[INFO|configuration_utils.py:581] 2021-11-29 16:02:06,663 >> loading configuration file ../output/config.json\n",
      "[INFO|configuration_utils.py:620] 2021-11-29 16:02:06,664 >> Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1321] 2021-11-29 16:02:06,665 >> loading weights file ../output/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1588] 2021-11-29 16:02:06,717 >> All model checkpoint weights were used when initializing AlbertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1597] 2021-11-29 16:02:06,718 >> All the weights of AlbertForQuestionAnswering were initialized from the model checkpoint at ../output/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForQuestionAnswering for predictions without further training.\n",
      "11/29/2021 16:02:06 - INFO - __main__ - Loading features from cached file .\\cached_dev_albert-base-v2_384\n",
      "Exception ignored in: <function tqdm.__del__ at 0x000001E4BD3CD708>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\bert\\lib\\site-packages\\tqdm\\std.py\", line 1147, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\bert\\lib\\site-packages\\tqdm\\notebook.py\", line 286, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n",
      "11/29/2021 16:02:12 - INFO - __main__ - ***** Running evaluation  *****\n",
      "11/29/2021 16:02:12 - INFO - __main__ -   Num examples = 12272\n",
      "11/29/2021 16:02:12 - INFO - __main__ -   Batch size = 64\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 192/192 [01:56<00:00,  1.65it/s]\n",
      "11/29/2021 16:04:09 - INFO - __main__ -   Evaluation done in total 116.361681 secs (0.009482 sec per example)\n",
      "[INFO|squad_metrics.py:401] 2021-11-29 16:04:09,099 >> Writing predictions to: ../output/predictions_.json\n",
      "[INFO|squad_metrics.py:403] 2021-11-29 16:04:09,100 >> Writing nbest to: ../output/nbest_predictions_.json\n",
      "[INFO|squad_metrics.py:405] 2021-11-29 16:04:09,100 >> Writing null_log_odds to: ../output/null_odds_.json\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading checkpoints saved during training for evaluation\")\n",
    "checkpoint = args.output_dir\n",
    "# Reload the model\n",
    "global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)  # , force_download=True)\n",
    "model.to(args.device)\n",
    "\n",
    "# Evaluate\n",
    "prefix=global_step\n",
    "dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
    "\n",
    "args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "# Note that DistributedSampler samples randomly\n",
    "eval_sampler = SequentialSampler(dataset)\n",
    "eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "# multi-gpu evaluate\n",
    "if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Eval!\n",
    "logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "logger.info(\"  Num examples = %d\", len(dataset))\n",
    "logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "\n",
    "all_results = []\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"token_type_ids\": batch[2],\n",
    "        }\n",
    "\n",
    "        if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\", \"bart\", \"longformer\"]:\n",
    "            del inputs[\"token_type_ids\"]\n",
    "\n",
    "        feature_indices = batch[3]\n",
    "\n",
    "        # XLNet and XLM use more arguments for their predictions\n",
    "        if args.model_type in [\"xlnet\", \"xlm\"]:\n",
    "            inputs.update({\"cls_index\": batch[4], \"p_mask\": batch[5]})\n",
    "            # for lang_id-sensitive xlm models\n",
    "            if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n",
    "                inputs.update(\n",
    "                    {\"langs\": (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)}\n",
    "                )\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    for i, feature_index in enumerate(feature_indices):\n",
    "        eval_feature = features[feature_index.item()]\n",
    "        unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "        output = [to_list(output[i]) for output in outputs.to_tuple()]\n",
    "\n",
    "        # Some models (XLNet, XLM) use 5 arguments for their predictions, while the other \"simpler\"\n",
    "        # models only use two.\n",
    "        if len(output) >= 5:\n",
    "            start_logits = output[0]\n",
    "            start_top_index = output[1]\n",
    "            end_logits = output[2]\n",
    "            end_top_index = output[3]\n",
    "            cls_logits = output[4]\n",
    "\n",
    "            result = SquadResult(\n",
    "                unique_id,\n",
    "                start_logits,\n",
    "                end_logits,\n",
    "                start_top_index=start_top_index,\n",
    "                end_top_index=end_top_index,\n",
    "                cls_logits=cls_logits,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            start_logits, end_logits = output\n",
    "            result = SquadResult(unique_id, start_logits, end_logits)\n",
    "\n",
    "        all_results.append(result)\n",
    "\n",
    "evalTime = timeit.default_timer() - start_time\n",
    "logger.info(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n",
    "\n",
    "# Compute predictions\n",
    "output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(prefix))\n",
    "output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
    "\n",
    "if args.version_2_with_negative:\n",
    "    output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds_{}.json\".format(prefix))\n",
    "else:\n",
    "    output_null_log_odds_file = None\n",
    "\n",
    "# XLNet and XLM use a more complex post-processing procedure\n",
    "if args.model_type in [\"xlnet\", \"xlm\"]:\n",
    "    start_n_top = model.config.start_n_top if hasattr(model, \"config\") else model.module.config.start_n_top\n",
    "    end_n_top = model.config.end_n_top if hasattr(model, \"config\") else model.module.config.end_n_top\n",
    "\n",
    "    predictions = compute_predictions_log_probs(\n",
    "        examples,\n",
    "        features,\n",
    "        all_results,\n",
    "        args.n_best_size,\n",
    "        args.max_answer_length,\n",
    "        output_prediction_file,\n",
    "        output_nbest_file,\n",
    "        output_null_log_odds_file,\n",
    "        start_n_top,\n",
    "        end_n_top,\n",
    "        args.version_2_with_negative,\n",
    "        tokenizer,\n",
    "        args.verbose_logging,\n",
    "    )\n",
    "else:\n",
    "    predictions = my_compute_predictions_logits(\n",
    "        examples,\n",
    "        features,\n",
    "        all_results,\n",
    "        args.n_best_size,\n",
    "        args.max_answer_length,\n",
    "        args.do_lower_case,\n",
    "        output_prediction_file,\n",
    "        output_nbest_file,\n",
    "        output_null_log_odds_file,\n",
    "        args.verbose_logging,\n",
    "        args.version_2_with_negative,\n",
    "        args.null_score_diff_threshold,\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "# Compute the F1 and exact scores.\n",
    "# result = squad_evaluate(examples, predictions)\n",
    "\n",
    "# result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n",
    "# logger.info(\"Results: {}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "73aa29b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('56ddde6b9a695914005b9628', 'France'),\n",
       "             ('56ddde6b9a695914005b9629', '10th and 11th centuries'),\n",
       "             ('56ddde6b9a695914005b962a', ''),\n",
       "             ('56ddde6b9a695914005b962b', ''),\n",
       "             ('56ddde6b9a695914005b962c', '10th'),\n",
       "             ('5ad39d53604f3c001a3fe8d1', ''),\n",
       "             ('5ad39d53604f3c001a3fe8d2', ''),\n",
       "             ('5ad39d53604f3c001a3fe8d3', ''),\n",
       "             ('5ad39d53604f3c001a3fe8d4', ''),\n",
       "             ('56dddf4066d3e219004dad5f', 'William the Conqueror'),\n",
       "             ('56dddf4066d3e219004dad60', 'Richard I of Normandy'),\n",
       "             ('56dddf4066d3e219004dad61', 'Christian'),\n",
       "             ('5ad3a266604f3c001a3fea27', ''),\n",
       "             ('5ad3a266604f3c001a3fea28', ''),\n",
       "             ('5ad3a266604f3c001a3fea29', ''),\n",
       "             ('5ad3a266604f3c001a3fea2a', 'Richard I'),\n",
       "             ('5ad3a266604f3c001a3fea2b', ''),\n",
       "             ('56dde0379a695914005b9636', 'plural of Normant'),\n",
       "             ('56dde0379a695914005b9637', '9th century'),\n",
       "             ('5ad3ab70604f3c001a3feb89', ''),\n",
       "             ('5ad3ab70604f3c001a3feb8a', '9th century'),\n",
       "             ('56dde0ba66d3e219004dad75', 'treaty of Saint-Clair-sur-Epte'),\n",
       "             ('56dde0ba66d3e219004dad76', 'King Charles III of West Francia'),\n",
       "             ('56dde0ba66d3e219004dad77', 'river Seine'),\n",
       "             ('5ad3ad61604f3c001a3fec0d', ''),\n",
       "             ('5ad3ad61604f3c001a3fec0e', ''),\n",
       "             ('5ad3ad61604f3c001a3fec0f', ''),\n",
       "             ('5ad3ad61604f3c001a3fec10', 'Viking incursions'),\n",
       "             ('56dde1d966d3e219004dad8d', 'Rollo'),\n",
       "             ('5ad3ae14604f3c001a3fec39', ''),\n",
       "             ('5ad3ae14604f3c001a3fec3a', ''),\n",
       "             ('56dde27d9a695914005b9651', ''),\n",
       "             ('56dde27d9a695914005b9652', 'north'),\n",
       "             ('5ad3af11604f3c001a3fec63', ''),\n",
       "             ('5ad3af11604f3c001a3fec64', ''),\n",
       "             ('5ad3af11604f3c001a3fec65', ''),\n",
       "             ('56dde2fa66d3e219004dad9b', 'fighting horsemen'),\n",
       "             ('5ad3c626604f3c001a3ff011', 'The Normans'),\n",
       "             ('5ad3c626604f3c001a3ff012', ''),\n",
       "             ('5ad3c626604f3c001a3ff013', ''),\n",
       "             ('56de0f6a4396321400ee257f',\n",
       "              'the Pechenegs, the Bulgars, and especially the Seljuk Turks.'),\n",
       "             ('5ad3dbc6604f3c001a3ff3e9', ''),\n",
       "             ('5ad3dbc6604f3c001a3ff3ea',\n",
       "              'the Pechenegs, the Bulgars, and especially the Seljuk Turks.'),\n",
       "             ('5ad3dbc6604f3c001a3ff3eb', 'Norman mercenaries'),\n",
       "             ('5ad3dbc6604f3c001a3ff3ec', 'George Maniaces'),\n",
       "             ('56de0ffd4396321400ee258d', '1050s'),\n",
       "             ('56de0ffd4396321400ee258e', '1060s'),\n",
       "             ('56de0ffd4396321400ee258f', 'Alexius Komnenos'),\n",
       "             ('5ad3de8b604f3c001a3ff467', 'Hervé'),\n",
       "             ('5ad3de8b604f3c001a3ff468', '1050s'),\n",
       "             ('5ad3de8b604f3c001a3ff469', ''),\n",
       "             ('5ad3de8b604f3c001a3ff46a', ''),\n",
       "             ('56de10b44396321400ee2593', 'Afranji'),\n",
       "             ('56de10b44396321400ee2594', 'Oursel'),\n",
       "             ('56de10b44396321400ee2595', 'Turkish forces'),\n",
       "             ('5ad3e96b604f3c001a3ff689', 'Normans joined Turkish'),\n",
       "             ('5ad3e96b604f3c001a3ff68a', ''),\n",
       "             ('5ad3e96b604f3c001a3ff68b', 'Oursel'),\n",
       "             ('5ad3e96b604f3c001a3ff68c',\n",
       "              'upper Euphrates valley in northern Syria'),\n",
       "             ('56de11154396321400ee25aa', 'an Italo-Norman named Raoul,'),\n",
       "             ('5ad3ea79604f3c001a3ff6e9', 'Byzantine Greece'),\n",
       "             ('5ad3ea79604f3c001a3ff6ea', ''),\n",
       "             ('5ad3ea79604f3c001a3ff6eb', ''),\n",
       "             ('56de148dcffd8e1900b4b5bc', 'Robert Guiscard'),\n",
       "             ('56de148dcffd8e1900b4b5bd', 'February 1082'),\n",
       "             ('56de148dcffd8e1900b4b5be', '30,000'),\n",
       "             ('5ad3ed26604f3c001a3ff799', ''),\n",
       "             ('5ad3ed26604f3c001a3ff79a', ''),\n",
       "             ('5ad3ed26604f3c001a3ff79b', ''),\n",
       "             ('5ad3ed26604f3c001a3ff79c', ''),\n",
       "             ('56de15104396321400ee25b7', 'Deabolis'),\n",
       "             ('56de15104396321400ee25b8', 'Bohemond'),\n",
       "             ('56de15104396321400ee25b9', 'river Deabolis'),\n",
       "             ('5ad3ee2d604f3c001a3ff7e1', ''),\n",
       "             ('5ad3ee2d604f3c001a3ff7e2', ''),\n",
       "             ('5ad3ee2d604f3c001a3ff7e3', 'Robert'),\n",
       "             ('56de1563cffd8e1900b4b5c2', '1185'),\n",
       "             ('56de1563cffd8e1900b4b5c3', 'Dyrrachium'),\n",
       "             ('56de1563cffd8e1900b4b5c4', ''),\n",
       "             ('5ad3f028604f3c001a3ff823', ''),\n",
       "             ('5ad3f028604f3c001a3ff824', ''),\n",
       "             ('5ad3f028604f3c001a3ff825', ''),\n",
       "             ('56de15dbcffd8e1900b4b5c8', ''),\n",
       "             ('56de15dbcffd8e1900b4b5c9', ''),\n",
       "             ('56de15dbcffd8e1900b4b5ca', 'Normandy'),\n",
       "             ('56de15dbcffd8e1900b4b5cb', 'Sweyn Forkbeard'),\n",
       "             ('5ad3f187604f3c001a3ff86f', ''),\n",
       "             ('5ad3f187604f3c001a3ff870', ''),\n",
       "             ('5ad3f187604f3c001a3ff871', ''),\n",
       "             ('56de1645cffd8e1900b4b5d0', 'Harthacnut'),\n",
       "             ('56de1645cffd8e1900b4b5d1', '1041'),\n",
       "             ('56de1645cffd8e1900b4b5d2', 'Robert of Jumièges'),\n",
       "             ('5ad3f350604f3c001a3ff8ef', '1041'),\n",
       "             ('5ad3f350604f3c001a3ff8f0', ''),\n",
       "             ('5ad3f350604f3c001a3ff8f1', ''),\n",
       "             ('56de16ca4396321400ee25c5', 'Battle of Hastings'),\n",
       "             ('56de16ca4396321400ee25c6', 'Duke William II of Normandy'),\n",
       "             ('56de16ca4396321400ee25c7', '1066'),\n",
       "             ('56de16ca4396321400ee25c8', 'Anglo-Saxons'),\n",
       "             ('5ad3f4b1604f3c001a3ff951', ''),\n",
       "             ('5ad3f4b1604f3c001a3ff952', ''),\n",
       "             ('5ad3f4b1604f3c001a3ff953', ''),\n",
       "             ('5ad3f4b1604f3c001a3ff954', 'Early Norman kings of England'),\n",
       "             ('56de1728cffd8e1900b4b5d7', ''),\n",
       "             ('5ad3f5b0604f3c001a3ff9ab', ''),\n",
       "             ('5ad3f5b0604f3c001a3ff9ac', ''),\n",
       "             ('5ad3f5b0604f3c001a3ff9ad', 'Geoffrey Chaucer'),\n",
       "             ('56de179dcffd8e1900b4b5da', '1169'),\n",
       "             ('56de179dcffd8e1900b4b5db', ''),\n",
       "             ('56de179dcffd8e1900b4b5dc', ''),\n",
       "             ('5ad3f6f5604f3c001a3ffa09', ''),\n",
       "             ('5ad3f6f5604f3c001a3ffa0a', ''),\n",
       "             ('5ad3f6f5604f3c001a3ffa0b', ''),\n",
       "             ('56de17f9cffd8e1900b4b5e0', ''),\n",
       "             ('56de17f9cffd8e1900b4b5e1', 'King Malcolm III of Scotland'),\n",
       "             ('56de17f9cffd8e1900b4b5e2', '1072'),\n",
       "             ('56de17f9cffd8e1900b4b5e3', 'Duncan'),\n",
       "             ('5ad3f7ac604f3c001a3ffa3b', ''),\n",
       "             ('5ad3f7ac604f3c001a3ffa3c', ''),\n",
       "             ('5ad3f7ac604f3c001a3ffa3d', 'Duncan'),\n",
       "             ('56de3cd0cffd8e1900b4b6be', 'Sybilla of Normandy'),\n",
       "             ('56de3cd0cffd8e1900b4b6bf', 'Normans and Norman'),\n",
       "             ('5ad3f8d2604f3c001a3ffa8d', 'Sybilla of Normandy'),\n",
       "             ('5ad3f8d2604f3c001a3ffa8e', ''),\n",
       "             ('56de3d594396321400ee26ca', 'Hereford'),\n",
       "             ('56de3d594396321400ee26cb', 'the Welsh'),\n",
       "             ('56de3d594396321400ee26cc', 'Edward the Confessor'),\n",
       "             ('5ad3fb01604f3c001a3ffb35', ''),\n",
       "             ('5ad3fb01604f3c001a3ffb36', ''),\n",
       "             ('56de3dbacffd8e1900b4b6d2', 'Wales'),\n",
       "             ('5ad3fb6e604f3c001a3ffb5f', ''),\n",
       "             ('5ad3fb6e604f3c001a3ffb60', ''),\n",
       "             ('56de3e414396321400ee26d8', '1018'),\n",
       "             ('56de3e414396321400ee26d9', 'William of Montreuil'),\n",
       "             ('5ad3fc41604f3c001a3ffb8f', 'Antioch'),\n",
       "             ('5ad3fc41604f3c001a3ffb90', ''),\n",
       "             ('5ad3fc41604f3c001a3ffb91', 'Roger de Tosny'),\n",
       "             ('5ad3fc41604f3c001a3ffb92', ''),\n",
       "             ('56de3ebc4396321400ee26e6', '1097'),\n",
       "             ('56de3ebc4396321400ee26e7', 'Tancred'),\n",
       "             ('56de3ebc4396321400ee26e8', 'Jerusalem'),\n",
       "             ('5ad4017a604f3c001a3ffd1f', '1097'),\n",
       "             ('5ad4017a604f3c001a3ffd20', ''),\n",
       "             ('56de3efccffd8e1900b4b6fe', '380 years'),\n",
       "             ('5ad401f2604f3c001a3ffd41', ''),\n",
       "             ('5ad401f2604f3c001a3ffd42', ''),\n",
       "             ('56de3f784396321400ee26fa', 'a storm'),\n",
       "             ('56de3f784396321400ee26fb', 'Berengaria'),\n",
       "             ('56de3f784396321400ee26fc', ''),\n",
       "             ('56de3f784396321400ee26fd', 'Isaac Komnenos'),\n",
       "             ('5ad40280604f3c001a3ffd57', ''),\n",
       "             ('5ad40280604f3c001a3ffd58', ''),\n",
       "             ('5ad40280604f3c001a3ffd59', ''),\n",
       "             ('56de40da4396321400ee2708', 'Conrad of Montferrat'),\n",
       "             ('56de40da4396321400ee2709', 'silver'),\n",
       "             ('56de40da4396321400ee270a', 'Guy de Lusignan'),\n",
       "             ('5ad404a6604f3c001a3ffde1', ''),\n",
       "             ('5ad404a6604f3c001a3ffde2', 'Guy de Lusignan'),\n",
       "             ('5ad404a6604f3c001a3ffde3', ''),\n",
       "             ('56de49564396321400ee277a', 'Africa'),\n",
       "             ('5ad40419604f3c001a3ffdb7', ''),\n",
       "             ('5ad40419604f3c001a3ffdb8', ''),\n",
       "             ('56de49a8cffd8e1900b4b7a7', 'Bethencourt'),\n",
       "             ('56de49a8cffd8e1900b4b7a8', 'Enrique Pérez de Guzmán'),\n",
       "             ('56de49a8cffd8e1900b4b7a9', 'Maciot de Bethencourt'),\n",
       "             ('5ad403c1604f3c001a3ffd97', ''),\n",
       "             ('5ad403c1604f3c001a3ffd98', ''),\n",
       "             ('56de4a474396321400ee2786', 'Channel Islands'),\n",
       "             ('56de4a474396321400ee2787', 'two'),\n",
       "             ('5ad40358604f3c001a3ffd7d', ''),\n",
       "             ('5ad40358604f3c001a3ffd7e', 'Très ancien coutumier'),\n",
       "             ('5ad40358604f3c001a3ffd7f', ''),\n",
       "             ('56de4a89cffd8e1900b4b7bd', 'Romanesque'),\n",
       "             ('56de4a89cffd8e1900b4b7be', 'rounded arches'),\n",
       "             ('5ad402ce604f3c001a3ffd67', 'rounded arches'),\n",
       "             ('56de4b074396321400ee2793', 'Early Gothic'),\n",
       "             ('56de4b074396321400ee2794', 'Early Gothic'),\n",
       "             ('56de4b074396321400ee2795', 'Sicily'),\n",
       "             ('5ad400b0604f3c001a3ffcdf', ''),\n",
       "             ('5ad400b0604f3c001a3ffce0', ''),\n",
       "             ('5ad400b0604f3c001a3ffce1', ''),\n",
       "             ('56de4b5c4396321400ee2799', 'early 11th century'),\n",
       "             ('56de4b5c4396321400ee279a', 'the dukes'),\n",
       "             ('5ad3ffd7604f3c001a3ffca7', ''),\n",
       "             ('5ad3ffd7604f3c001a3ffca8', ''),\n",
       "             ('5ad3ffd7604f3c001a3ffca9', ''),\n",
       "             ('5ad3ffd7604f3c001a3ffcaa', ''),\n",
       "             ('56de4bb84396321400ee27a2', '16th century'),\n",
       "             ('5ad3ff1b604f3c001a3ffc73', ''),\n",
       "             ('5ad3ff1b604f3c001a3ffc74', ''),\n",
       "             ('56de4c324396321400ee27ab', 'embroidery'),\n",
       "             ('56de4c324396321400ee27ac', 'Bayeux Tapestry'),\n",
       "             ('56de4c324396321400ee27ad',\n",
       "              'Odo, the Bishop of Bayeux and first Earl of Kent,'),\n",
       "             ('5ad3fe91604f3c001a3ffc47', ''),\n",
       "             ('5ad3fe91604f3c001a3ffc48', ''),\n",
       "             ('56de51244396321400ee27ef', 'mosaics'),\n",
       "             ('5ad3fe0d604f3c001a3ffc1b', ''),\n",
       "             ('5ad3fe0d604f3c001a3ffc1c', 'mosaics'),\n",
       "             ('5ad3fe0d604f3c001a3ffc1d', ''),\n",
       "             ('56de51c64396321400ee27f7', '11th'),\n",
       "             ('56de51c64396321400ee27f8',\n",
       "              'William of Volpiano and John of Ravenna'),\n",
       "             ('5ad3fd68604f3c001a3ffbe7', ''),\n",
       "             ('5ad3fd68604f3c001a3ffbe8', 'musical production and education'),\n",
       "             ('56de52614396321400ee27fb', 'southern Italy'),\n",
       "             ('56de52614396321400ee27fc', \"Sant'Eufemia.\"),\n",
       "             ('56de52614396321400ee27fd', 'Robert Guiscard'),\n",
       "             ('56de52614396321400ee27fe', 'singing'),\n",
       "             ('5ad3fccf604f3c001a3ffbb5', ''),\n",
       "             ('56e16182e3433e1400422e28', 'Computational complexity theory'),\n",
       "             ('56e16182e3433e1400422e29', 'inherent difficulty'),\n",
       "             ('56e16182e3433e1400422e2a', 'computational problem'),\n",
       "             ('5ad5316b5b96ef001a10ab72', ''),\n",
       "             ('5ad5316b5b96ef001a10ab73', 'Computational complexity theory'),\n",
       "             ('5ad5316b5b96ef001a10ab74', ''),\n",
       "             ('5ad5316b5b96ef001a10ab75', ''),\n",
       "             ('5ad5316b5b96ef001a10ab76', ''),\n",
       "             ('56e16839cd28a01900c67887',\n",
       "              'if its solution requires significant resources'),\n",
       "             ('56e16839cd28a01900c67888',\n",
       "              'mathematical models of computation'),\n",
       "             ('56e16839cd28a01900c67889', 'time and storage'),\n",
       "             ('56e16839cd28a01900c6788a', 'number of gates in a circuit'),\n",
       "             ('56e16839cd28a01900c6788b',\n",
       "              'to determine the practical limits on what computers can and cannot do'),\n",
       "             ('5ad532575b96ef001a10ab7c', ''),\n",
       "             ('5ad532575b96ef001a10ab7d', ''),\n",
       "             ('5ad532575b96ef001a10ab7e', ''),\n",
       "             ('5ad532575b96ef001a10ab7f', ''),\n",
       "             ('5ad532575b96ef001a10ab80', ''),\n",
       "             ('56e17644e3433e1400422f40',\n",
       "              'analysis of algorithms and computability theory'),\n",
       "             ('56e17644e3433e1400422f41', 'computational complexity theory'),\n",
       "             ('56e17644e3433e1400422f42', 'computational complexity theory'),\n",
       "             ('56e17644e3433e1400422f43', 'computational complexity theory'),\n",
       "             ('5ad5344b5b96ef001a10ab86', ''),\n",
       "             ('5ad5344b5b96ef001a10ab87', ''),\n",
       "             ('5ad5344b5b96ef001a10ab88', ''),\n",
       "             ('5ad5344b5b96ef001a10ab89', ''),\n",
       "             ('5ad5344b5b96ef001a10ab8a', ''),\n",
       "             ('56e17a7ccd28a01900c679a1', 'problem instance'),\n",
       "             ('56e17a7ccd28a01900c679a2', 'a problem'),\n",
       "             ('56e17a7ccd28a01900c679a3', 'concrete'),\n",
       "             ('56e17a7ccd28a01900c679a4', 'solution is the output'),\n",
       "             ('56e17a7ccd28a01900c679a5', 'the solution'),\n",
       "             ('5ad5364c5b96ef001a10ab90', ''),\n",
       "             ('5ad5364c5b96ef001a10ab91', 'problem instance'),\n",
       "             ('5ad5364c5b96ef001a10ab92', ''),\n",
       "             ('5ad5364c5b96ef001a10ab93', ''),\n",
       "             ('5ad5364c5b96ef001a10ab94', ''),\n",
       "             ('56e17e6ee3433e1400422f7f', '2000'),\n",
       "             ('56e17e6ee3433e1400422f80',\n",
       "              'asking for a round trip through all sites in Milan whose total length is at most 10 km'),\n",
       "             ('56e17e6ee3433e1400422f81', ''),\n",
       "             ('5ad537a15b96ef001a10ab9a', ''),\n",
       "             ('5ad537a15b96ef001a10ab9b', ''),\n",
       "             ('5ad537a15b96ef001a10ab9c',\n",
       "              'asking for a round trip through all sites in Milan whose total length is at most 10 km'),\n",
       "             ('5ad537a15b96ef001a10ab9d', ''),\n",
       "             ('56e181d9e3433e1400422fa0', 'a problem instance'),\n",
       "             ('56e181d9e3433e1400422fa1', 'binary alphabet'),\n",
       "             ('56e181d9e3433e1400422fa2', 'a string over an alphabet'),\n",
       "             ('56e181d9e3433e1400422fa3', 'binary notation'),\n",
       "             ('56e181d9e3433e1400422fa4', 'via their adjacency matrices'),\n",
       "             ('5ad5391e5b96ef001a10aba2', ''),\n",
       "             ('5ad5391e5b96ef001a10aba3', ''),\n",
       "             ('5ad5391e5b96ef001a10aba4', ''),\n",
       "             ('5ad5391e5b96ef001a10aba5', ''),\n",
       "             ('5ad5391e5b96ef001a10aba6', ''),\n",
       "             ('56e190bce3433e1400422fc8', 'Decision problems'),\n",
       "             ('56e190bce3433e1400422fc9', '1 or 0'),\n",
       "             ('56e190bce3433e1400422fca', '1 or 0'),\n",
       "             ('56e190bce3433e1400422fcb', 'yes'),\n",
       "             ('56e190bce3433e1400422fcc', ''),\n",
       "             ('5ad53b9d5b96ef001a10abc8', 'Decision problems'),\n",
       "             ('5ad53b9d5b96ef001a10abc9', 'decision problem'),\n",
       "             ('5ad53b9d5b96ef001a10abca', ''),\n",
       "             ('5ad53b9d5b96ef001a10abcb', '1 or 0'),\n",
       "             ('5ad53b9d5b96ef001a10abcc',\n",
       "              'If the algorithm deciding this problem returns the answer yes, the algorithm is said to accept the input string,'),\n",
       "             ('56e19557e3433e1400422fee', 'arbitrary'),\n",
       "             ('56e19557e3433e1400422ff0', 'formal language'),\n",
       "             ('56e19557e3433e1400422ff1', ''),\n",
       "             ('5ad53d705b96ef001a10abd2', ''),\n",
       "             ('5ad53d705b96ef001a10abd3', ''),\n",
       "             ('5ad53d705b96ef001a10abd4', ''),\n",
       "             ('5ad53d705b96ef001a10abd5', ''),\n",
       "             ('56e19724cd28a01900c679f6', 'computational problem'),\n",
       "             ('56e19724cd28a01900c679f7', 'a single output'),\n",
       "             ('56e19724cd28a01900c679f8', 'function'),\n",
       "             ('56e19724cd28a01900c679f9', 'integer factorization problem'),\n",
       "             ('56e19724cd28a01900c679fa', 'complex'),\n",
       "             ('5ad53e615b96ef001a10abda', ''),\n",
       "             ('5ad53e615b96ef001a10abdb', ''),\n",
       "             ('5ad53e615b96ef001a10abdc', ''),\n",
       "             ('5ad53e615b96ef001a10abdd', ''),\n",
       "             ('5ad53e615b96ef001a10abde', 'complex'),\n",
       "             ('56e1a0dccd28a01900c67a2e', 'decision problems'),\n",
       "             ('56e1a0dccd28a01900c67a2f', 'the set of triples'),\n",
       "             ('5ad53f815b96ef001a10abe4', ''),\n",
       "             ('5ad53f815b96ef001a10abe5', ''),\n",
       "             ('5ad53f815b96ef001a10abe6', ''),\n",
       "             ('56e1a38de3433e140042305c', ''),\n",
       "             ('56e1a38de3433e140042305d', 'the instance'),\n",
       "             ('56e1a38de3433e140042305e',\n",
       "              'as a function of the size of the instance'),\n",
       "             ('56e1a38de3433e140042305f', ''),\n",
       "             ('56e1a38de3433e1400423060', 'input size'),\n",
       "             ('5ad541ad5b96ef001a10abea', ''),\n",
       "             ('5ad541ad5b96ef001a10abeb', ''),\n",
       "             ('5ad541ad5b96ef001a10abec',\n",
       "              'as a function of the size of the instance'),\n",
       "             ('5ad541ad5b96ef001a10abed', ''),\n",
       "             ('5ad541ad5b96ef001a10abee', ''),\n",
       "             ('56e1a564cd28a01900c67a48', \"Cobham's\"),\n",
       "             ('56e1a564cd28a01900c67a49', 'the time taken'),\n",
       "             ('56e1a564cd28a01900c67a4a', 'worst-case time complexity T(n)'),\n",
       "             ('56e1a564cd28a01900c67a4b', 'T(n)'),\n",
       "             ('56e1a564cd28a01900c67a4c', 'polynomial time algorithm'),\n",
       "             ('5ad542db5b96ef001a10abf4', ''),\n",
       "             ('5ad542db5b96ef001a10abf5', ''),\n",
       "             ('5ad542db5b96ef001a10abf6', ''),\n",
       "             ('5ad542db5b96ef001a10abf7', ''),\n",
       "             ('5ad542db5b96ef001a10abf8', ''),\n",
       "             ('56e1aba0e3433e1400423094', 'A Turing machine'),\n",
       "             ('56e1aba0e3433e1400423095', 'an algorithm'),\n",
       "             ('56e1aba0e3433e1400423097', 'the Turing machine'),\n",
       "             ('56e1aba0e3433e1400423098', 'symbols'),\n",
       "             ('5ad543c05b96ef001a10abfe', ''),\n",
       "             ('5ad543c05b96ef001a10abff', ''),\n",
       "             ('5ad543c05b96ef001a10ac00', ''),\n",
       "             ('5ad543c05b96ef001a10ac01', ''),\n",
       "             ('56e1aff7cd28a01900c67a68', 'A deterministic'),\n",
       "             ('56e1aff7cd28a01900c67a69',\n",
       "              'uses a fixed set of rules to determine its future actions'),\n",
       "             ('56e1aff7cd28a01900c67a6a', 'probabilistic'),\n",
       "             ('56e1aff7cd28a01900c67a6b', ''),\n",
       "             ('56e1aff7cd28a01900c67a6c', 'randomized algorithms'),\n",
       "             ('5ad546c75b96ef001a10ac0e', ''),\n",
       "             ('5ad546c75b96ef001a10ac0f', ''),\n",
       "             ('5ad546c75b96ef001a10ac10', ''),\n",
       "             ('5ad546c75b96ef001a10ac11', ''),\n",
       "             ('5ad546c75b96ef001a10ac12', ''),\n",
       "             ('56e1b00ce3433e140042309e', 'complexity classes'),\n",
       "             ('56e1b00ce3433e140042309f', ''),\n",
       "             ('56e1b00ce3433e14004230a1',\n",
       "              'deterministic Turing machines, probabilistic Turing machines,'),\n",
       "             ('5ad545545b96ef001a10ac06', ''),\n",
       "             ('5ad545545b96ef001a10ac07', ''),\n",
       "             ('5ad545545b96ef001a10ac08', ''),\n",
       "             ('5ad545545b96ef001a10ac09', ''),\n",
       "             ('56e1b169cd28a01900c67a72', 'random access machines'),\n",
       "             ('56e1b169cd28a01900c67a73', 'computational power'),\n",
       "             ('56e1b169cd28a01900c67a74', 'time and memory'),\n",
       "             ('56e1b169cd28a01900c67a75',\n",
       "              'the machines operate deterministically'),\n",
       "             ('5ad547945b96ef001a10ac18', ''),\n",
       "             ('5ad547945b96ef001a10ac19', 'memory consumption'),\n",
       "             ('5ad547945b96ef001a10ac1a', ''),\n",
       "             ('5ad547945b96ef001a10ac1b', ''),\n",
       "             ('56e1b355e3433e14004230b0', 'non-deterministic'),\n",
       "             ('56e1b355e3433e14004230b1', 'unusual resources'),\n",
       "             ('56e1b355e3433e14004230b2', 'mathematical models'),\n",
       "             ('56e1b355e3433e14004230b3', 'non-deterministic time'),\n",
       "             ('5ad5489b5b96ef001a10ac2a', ''),\n",
       "             ('5ad5489b5b96ef001a10ac2b', ''),\n",
       "             ('5ad5489b5b96ef001a10ac2c', ''),\n",
       "             ('5ad5489b5b96ef001a10ac2d', ''),\n",
       "             ('5ad5489b5b96ef001a10ac2e', ''),\n",
       "             ('56e1b62ecd28a01900c67aa3', ''),\n",
       "             ('56e1b62ecd28a01900c67aa4', 'difficulty'),\n",
       "             ('56e1b62ecd28a01900c67aa5', 'DTIME(f(n)).'),\n",
       "             ('56e1b62ecd28a01900c67aa6', 'computational model'),\n",
       "             ('5ad54a375b96ef001a10ac48', ''),\n",
       "             ('5ad54a375b96ef001a10ac49', ''),\n",
       "             ('5ad54a375b96ef001a10ac4a', 'DTIME(f(n)).'),\n",
       "             ('5ad54a375b96ef001a10ac4b', ''),\n",
       "             ('5ad54a375b96ef001a10ac4c', ''),\n",
       "             ('56e1b754cd28a01900c67abc', 'complexity'),\n",
       "             ('56e1b754cd28a01900c67abd', 'computational'),\n",
       "             ('56e1b754cd28a01900c67abe', 'Blum complexity axioms'),\n",
       "             ('56e1b754cd28a01900c67abf', 'complexity'),\n",
       "             ('56e1b754cd28a01900c67ac0', 'complexity'),\n",
       "             ('5ad54b035b96ef001a10ac52', ''),\n",
       "             ('5ad54b035b96ef001a10ac53', ''),\n",
       "             ('5ad54b035b96ef001a10ac54', ''),\n",
       "             ('5ad54b035b96ef001a10ac55', ''),\n",
       "             ('5ad54b035b96ef001a10ac56', ''),\n",
       "             ('56e1b8f3e3433e14004230e6',\n",
       "              'The best, worst and average case complexity'),\n",
       "             ('56e1b8f3e3433e14004230e7', ''),\n",
       "             ('56e1b8f3e3433e14004230e8', 'time complexity'),\n",
       "             ('56e1b8f3e3433e14004230e9', ''),\n",
       "             ('5ad54c2f5b96ef001a10ac5c', ''),\n",
       "             ('5ad54c2f5b96ef001a10ac5d', ''),\n",
       "             ('5ad54c2f5b96ef001a10ac5e', ''),\n",
       "             ('5ad54c2f5b96ef001a10ac5f', ''),\n",
       "             ('56e1ba41cd28a01900c67ae0',\n",
       "              'deterministic sorting algorithm quicksort'),\n",
       "             ('56e1ba41cd28a01900c67ae1', 'The worst-case'),\n",
       "             ('56e1ba41cd28a01900c67ae2', ''),\n",
       "             ('5ad54d625b96ef001a10ac64', ''),\n",
       "             ('5ad54d625b96ef001a10ac65',\n",
       "              'solves the problem of sorting a list of integers that is given as the input'),\n",
       "             ('5ad54d625b96ef001a10ac66', ''),\n",
       "             ('5ad54d625b96ef001a10ac67', ''),\n",
       "             ('5ad54d625b96ef001a10ac68', ''),\n",
       "             ('56e1bc3ae3433e1400423104', 'the most efficient algorithm'),\n",
       "             ('56e1bc3ae3433e1400423105', 'analysis of algorithms'),\n",
       "             ('56e1bc3ae3433e1400423106', 'lower bounds'),\n",
       "             ('56e1bc3ae3433e1400423107', 'upper bound'),\n",
       "             ('56e1bc3ae3433e1400423108', '\"all possible algorithms\"'),\n",
       "             ('5ad54e7c5b96ef001a10ac76',\n",
       "              'proving upper and lower bounds on the minimum amount of time required by the most efficient algorithm solving a given problem'),\n",
       "             ('5ad54e7c5b96ef001a10ac77', ''),\n",
       "             ('5ad54e7c5b96ef001a10ac78', ''),\n",
       "             ('5ad54e7c5b96ef001a10ac79', ''),\n",
       "             ('5ad54e7c5b96ef001a10ac7a', ''),\n",
       "             ('56e1bd4acd28a01900c67afc', 'big O notation'),\n",
       "             ('56e1bd4acd28a01900c67afd',\n",
       "              'constant factors and smaller terms'),\n",
       "             ('56e1bd4acd28a01900c67afe', ''),\n",
       "             ('56e1bd4acd28a01900c67aff', ''),\n",
       "             ('5ad54f775b96ef001a10ac88', ''),\n",
       "             ('5ad54f775b96ef001a10ac89', ''),\n",
       "             ('5ad54f775b96ef001a10ac8a', ''),\n",
       "             ('5ad54f775b96ef001a10ac8b', ''),\n",
       "             ('56e1c0f6cd28a01900c67b2c', 'complexity classes'),\n",
       "             ('56e1c0f6cd28a01900c67b2d', ''),\n",
       "             ('56e1c0f6cd28a01900c67b2e', 'complicated definitions'),\n",
       "             ('5ad5501f5b96ef001a10ac90', ''),\n",
       "             ('5ad5501f5b96ef001a10ac91', ''),\n",
       "             ('5ad5501f5b96ef001a10ac92', ''),\n",
       "             ('5ad5501f5b96ef001a10ac93', ''),\n",
       "             ('56e1c2eee3433e1400423134', 'the chosen machine model'),\n",
       "             ('56e1c2eee3433e1400423135', 'quadratic time'),\n",
       "             ('56e1c2eee3433e1400423136', 'single-tape'),\n",
       "             ('56e1c2eee3433e1400423137', 'Cobham-Edmonds'),\n",
       "             ('56e1c2eee3433e1400423138', 'complexity class P'),\n",
       "             ('5ad55ee35b96ef001a10ace4', ''),\n",
       "             ('5ad55ee35b96ef001a10ace5', ''),\n",
       "             ('5ad55ee35b96ef001a10ace6', ''),\n",
       "             ('5ad55ee35b96ef001a10ace7', ''),\n",
       "             ('5ad55ee35b96ef001a10ace8', ''),\n",
       "             ('56e1c3e1e3433e1400423148', 'time or space'),\n",
       "             ('56e1c3e1e3433e1400423149', ''),\n",
       "             ('56e1c3e1e3433e140042314a', 'complexity classes'),\n",
       "             ('5ad55fe75b96ef001a10ad0c', ''),\n",
       "             ('5ad55fe75b96ef001a10ad0d', ''),\n",
       "             ('5ad55fe75b96ef001a10ad0e', ''),\n",
       "             ('5ad55fe75b96ef001a10ad0f', ''),\n",
       "             ('56e1c4fce3433e140042314e', 'BPP, ZPP and RP,'),\n",
       "             ('56e1c4fce3433e140042314f', 'Boolean'),\n",
       "             ('56e1c4fce3433e1400423150', 'quantum'),\n",
       "             ('56e1c4fce3433e1400423151',\n",
       "              'BQP and QMA, which are defined using quantum Turing machines. #P'),\n",
       "             ('56e1c4fce3433e1400423152', 'Interactive'),\n",
       "             ('5ad560b85b96ef001a10ad1e', ''),\n",
       "             ('5ad560b85b96ef001a10ad1f', ''),\n",
       "             ('5ad560b85b96ef001a10ad20', ''),\n",
       "             ('5ad560b85b96ef001a10ad21', ''),\n",
       "             ('5ad560b85b96ef001a10ad22', ''),\n",
       "             ('56e1c720e3433e140042316a', 'computation time'),\n",
       "             ('56e1c720e3433e140042316b', 'DTIME(n2),'),\n",
       "             ('56e1c720e3433e140042316c', 'time and space hierarchy theorems'),\n",
       "             ('56e1c720e3433e140042316d', 'proper hierarchy'),\n",
       "             ('56e1c720e3433e140042316e', 'quantitative'),\n",
       "             ('5ad561c85b96ef001a10ad3c', ''),\n",
       "             ('5ad561c85b96ef001a10ad3d', ''),\n",
       "             ('5ad561c85b96ef001a10ad3e', ''),\n",
       "             ('5ad561c85b96ef001a10ad3f', ''),\n",
       "             ('5ad561c85b96ef001a10ad40', ''),\n",
       "             ('56e1c7e2cd28a01900c67b74',\n",
       "              'The time and space hierarchy theorems'),\n",
       "             ('56e1c7e2cd28a01900c67b75', 'strictly contained in EXPTIME'),\n",
       "             ('56e1c7e2cd28a01900c67b76', 'PSPACE'),\n",
       "             ('5ad562525b96ef001a10ad50', ''),\n",
       "             ('5ad562525b96ef001a10ad51', ''),\n",
       "             ('5ad562525b96ef001a10ad52', ''),\n",
       "             ('5ad562525b96ef001a10ad53', ''),\n",
       "             ('56e1c9bfe3433e1400423192', 'a reduction'),\n",
       "             ('56e1c9bfe3433e1400423193', 'another problem'),\n",
       "             ('56e1c9bfe3433e1400423194', 'reduces'),\n",
       "             ('56e1c9bfe3433e1400423195',\n",
       "              'Cook reductions, Karp reductions and Levin reductions,'),\n",
       "             ('56e1c9bfe3433e1400423196', 'different types of reductions'),\n",
       "             ('5ad5632f5b96ef001a10ad6c', ''),\n",
       "             ('5ad5632f5b96ef001a10ad6d', ''),\n",
       "             ('5ad5632f5b96ef001a10ad6e', ''),\n",
       "             ('5ad5632f5b96ef001a10ad6f', ''),\n",
       "             ('5ad5632f5b96ef001a10ad70', ''),\n",
       "             ('56e1cbe2cd28a01900c67bac', 'polynomial-time reduction.'),\n",
       "             ('56e1cbe2cd28a01900c67bad', ''),\n",
       "             ('56e1cbe2cd28a01900c67bae', 'polynomial'),\n",
       "             ('56e1cbe2cd28a01900c67baf',\n",
       "              'giving the same input to both inputs'),\n",
       "             ('56e1cbe2cd28a01900c67bb0', 'multiplication'),\n",
       "             ('5ad5648b5b96ef001a10ad94', ''),\n",
       "             ('5ad5648b5b96ef001a10ad95', ''),\n",
       "             ('5ad5648b5b96ef001a10ad96', ''),\n",
       "             ('5ad5648b5b96ef001a10ad97', ''),\n",
       "             ('5ad5648b5b96ef001a10ad98', ''),\n",
       "             ('56e1ce08e3433e14004231a4', ''),\n",
       "             ('56e1ce08e3433e14004231a5', ''),\n",
       "             ('56e1ce08e3433e14004231a6', ''),\n",
       "             ('56e1ce08e3433e14004231a8', 'the set of NP-hard problems.'),\n",
       "             ('5ad565575b96ef001a10adb2', ''),\n",
       "             ('5ad565575b96ef001a10adb3', ''),\n",
       "             ('5ad565575b96ef001a10adb4', 'no'),\n",
       "             ('5ad565575b96ef001a10adb5', ''),\n",
       "             ('56e1d9fee3433e14004231cb', 'NP-complete'),\n",
       "             ('56e1d9fee3433e14004231cc', ''),\n",
       "             ('56e1d9fee3433e14004231cd',\n",
       "              'there is no known polynomial-time solution for Π1.'),\n",
       "             ('56e1d9fee3433e14004231ce', 'NP'),\n",
       "             ('5ad566375b96ef001a10adce', ''),\n",
       "             ('5ad566375b96ef001a10adcf', ''),\n",
       "             ('5ad566375b96ef001a10add0', ''),\n",
       "             ('5ad566375b96ef001a10add1', ''),\n",
       "             ('56e1dc62cd28a01900c67bca', 'P'),\n",
       "             ('56e1dc62cd28a01900c67bcb', 'Cobham–Edmonds thesis.'),\n",
       "             ('56e1dc62cd28a01900c67bcc', ''),\n",
       "             ('56e1dc62cd28a01900c67bcd', 'Boolean satisfiability problem'),\n",
       "             ('56e1dc62cd28a01900c67bce', 'deterministic Turing machines'),\n",
       "             ('5ad567055b96ef001a10adea', ''),\n",
       "             ('5ad567055b96ef001a10adeb', ''),\n",
       "             ('5ad567055b96ef001a10adec', ''),\n",
       "             ('5ad567055b96ef001a10aded', ''),\n",
       "             ('5ad567055b96ef001a10adee', ''),\n",
       "             ('56e1ddfce3433e14004231d5', 'more efficient solutions'),\n",
       "             ('56e1ddfce3433e14004231d6', 'protein structure prediction'),\n",
       "             ('56e1ddfce3433e14004231d8', 'US$1,000,000'),\n",
       "             ('5ad568175b96ef001a10ae10', ''),\n",
       "             ('5ad568175b96ef001a10ae11', ''),\n",
       "             ('5ad568175b96ef001a10ae12', ''),\n",
       "             ('5ad568175b96ef001a10ae13', ''),\n",
       "             ('5ad568175b96ef001a10ae14', ''),\n",
       "             ('56e1ded7cd28a01900c67bd4', 'Ladner'),\n",
       "             ('56e1ded7cd28a01900c67bd5', 'NP-intermediate problems.'),\n",
       "             ('56e1ded7cd28a01900c67bd6', 'The graph isomorphism problem'),\n",
       "             ('5ad568d35b96ef001a10ae1a', ''),\n",
       "             ('5ad568d35b96ef001a10ae1b', ''),\n",
       "             ('5ad568d35b96ef001a10ae1c', ''),\n",
       "             ('5ad568d35b96ef001a10ae1d', ''),\n",
       "             ('56e1e9dfe3433e14004231fc', 'graph isomorphism'),\n",
       "             ('56e1e9dfe3433e14004231fd', ''),\n",
       "             ('56e1e9dfe3433e14004231fe', ''),\n",
       "             ('56e1e9dfe3433e14004231ff', 'second'),\n",
       "             ('56e1e9dfe3433e1400423200', 'Laszlo Babai and Eugene Luks'),\n",
       "             ('5ad569c05b96ef001a10ae36', ''),\n",
       "             ('5ad569c05b96ef001a10ae37', ''),\n",
       "             ('5ad569c05b96ef001a10ae38', ''),\n",
       "             ('5ad569c05b96ef001a10ae39', ''),\n",
       "             ('5ad569c05b96ef001a10ae3a', ''),\n",
       "             ('56e1ec83cd28a01900c67c0a', ''),\n",
       "             ('56e1ec83cd28a01900c67c0b', 'k'),\n",
       "             ('56e1ec83cd28a01900c67c0c', 'the RSA algorithm'),\n",
       "             ('56e1ec83cd28a01900c67c0e', 'general number field sieve'),\n",
       "             ('5ad56aea5b96ef001a10ae48', ''),\n",
       "             ('5ad56aea5b96ef001a10ae49', ''),\n",
       "             ('5ad56aea5b96ef001a10ae4a', ''),\n",
       "             ('5ad56aea5b96ef001a10ae4b', ''),\n",
       "             ('5ad56aea5b96ef001a10ae4c', ''),\n",
       "             ('56e1ee4de3433e1400423210', 'unequal'),\n",
       "             ('56e1ee4de3433e1400423211', 'P = PSPACE'),\n",
       "             ('56e1ee4de3433e1400423212', 'between P and PSPACE'),\n",
       "             ('56e1ee4de3433e1400423214',\n",
       "              'Proving that any of these classes are unequal'),\n",
       "             ('5ad56bcd5b96ef001a10ae62', 'unequal'),\n",
       "             ('5ad56bcd5b96ef001a10ae63', ''),\n",
       "             ('5ad56bcd5b96ef001a10ae64', ''),\n",
       "             ('5ad56bcd5b96ef001a10ae65', ''),\n",
       "             ('5ad56bcd5b96ef001a10ae66', ''),\n",
       "             ('56e1efa0e3433e140042321a', 'co-NP'),\n",
       "             ('56e1efa0e3433e140042321b',\n",
       "              'co-NP is the class containing the complement problems (i.e. problems with the yes/no answers reversed)'),\n",
       "             ('56e1efa0e3433e140042321c', 'P is not equal to NP'),\n",
       "             ('56e1efa0e3433e140042321d', ''),\n",
       "             ('5ad56c6b5b96ef001a10ae6c', ''),\n",
       "             ('5ad56c6b5b96ef001a10ae6d', 'reversed'),\n",
       "             ('5ad56c6b5b96ef001a10ae6e', ''),\n",
       "             ('5ad56c6b5b96ef001a10ae6f', ''),\n",
       "             ('56e1f10ee3433e1400423222', ''),\n",
       "             ('56e1f10ee3433e1400423223', 'equal to P'),\n",
       "             ('56e1f10ee3433e1400423224', ''),\n",
       "             ('56e1f10ee3433e1400423225', ''),\n",
       "             ('56e1f10ee3433e1400423226', ''),\n",
       "             ('5ad56d3e5b96ef001a10ae84', ''),\n",
       "             ('5ad56d3e5b96ef001a10ae85', ''),\n",
       "             ('5ad56d3e5b96ef001a10ae86', ''),\n",
       "             ('5ad56d3e5b96ef001a10ae87', ''),\n",
       "             ('5ad56d3e5b96ef001a10ae88', ''),\n",
       "             ('56e1fc57e3433e140042322c', 'intractable problems'),\n",
       "             ('56e1fc57e3433e140042322f', 'polynomial time'),\n",
       "             ('56e1fc57e3433e1400423230', 'NP-complete problems'),\n",
       "             ('5ad56e5c5b96ef001a10ae9e', 'intractable problems'),\n",
       "             ('5ad56e5c5b96ef001a10ae9f', ''),\n",
       "             ('5ad56e5c5b96ef001a10aea0', ''),\n",
       "             ('5ad56e5c5b96ef001a10aea1', ''),\n",
       "             ('5ad56e5c5b96ef001a10aea2', ''),\n",
       "             ('56e1febfe3433e1400423236', 'Presburger arithmetic'),\n",
       "             ('56e1febfe3433e1400423237', 'algorithms'),\n",
       "             ('56e1febfe3433e1400423238', 'NP-complete knapsack problem'),\n",
       "             ('56e1febfe3433e1400423239', 'less than quadratic time'),\n",
       "             ('56e1febfe3433e140042323a', 'NP-complete knapsack'),\n",
       "             ('5ad56ef05b96ef001a10aea8', ''),\n",
       "             ('5ad56ef05b96ef001a10aea9', ''),\n",
       "             ('5ad56ef05b96ef001a10aeaa', ''),\n",
       "             ('5ad56ef05b96ef001a10aeab', ''),\n",
       "             ('56e200e4cd28a01900c67c14', ''),\n",
       "             ('56e200e4cd28a01900c67c15', 'Alan Turing'),\n",
       "             ('56e200e4cd28a01900c67c16', 'Turing machines'),\n",
       "             ('56e200e4cd28a01900c67c17', '1936'),\n",
       "             ('56e200e4cd28a01900c67c18', 'a computer'),\n",
       "             ('5ad56fe65b96ef001a10aec2', ''),\n",
       "             ('5ad56fe65b96ef001a10aec3', ''),\n",
       "             ('5ad56fe65b96ef001a10aec4', ''),\n",
       "             ('5ad56fe65b96ef001a10aec5', ''),\n",
       "             ('5ad56fe65b96ef001a10aec6', ''),\n",
       "             ('56e202e9e3433e1400423240',\n",
       "              '\"On the Computational Complexity of Algorithms\"'),\n",
       "             ('56e202e9e3433e1400423241',\n",
       "              'Juris Hartmanis and Richard Stearns'),\n",
       "             ('56e202e9e3433e1400423242', '(1965),'),\n",
       "             ('56e202e9e3433e1400423243', 'time and space complexity'),\n",
       "             ('56e202e9e3433e1400423244', '1965'),\n",
       "             ('5ad570b25b96ef001a10aedc', ''),\n",
       "             ('5ad570b25b96ef001a10aedd', ''),\n",
       "             ('5ad570b25b96ef001a10aede', ''),\n",
       "             ('5ad570b25b96ef001a10aedf', ''),\n",
       "             ('56e2042ecd28a01900c67c1e', 'John Myhill'),\n",
       "             ('56e2042ecd28a01900c67c1f', '(1961),'),\n",
       "             ('56e2042ecd28a01900c67c20', 'Hisao Yamada'),\n",
       "             ('5ad5719f5b96ef001a10aeec', ''),\n",
       "             ('5ad5719f5b96ef001a10aeed', ''),\n",
       "             ('5ad5719f5b96ef001a10aeee', ''),\n",
       "             ('5ad5719f5b96ef001a10aeef', ''),\n",
       "             ('5ad5719f5b96ef001a10aef0', ''),\n",
       "             ('56e20a27cd28a01900c67c24', 'input encoding'),\n",
       "             ('56e20a27cd28a01900c67c25', 'encoding'),\n",
       "             ('5ad53ac65b96ef001a10abac', ''),\n",
       "             ('5ad53ac65b96ef001a10abad', ''),\n",
       "             ('5ad53ac65b96ef001a10abae', ''),\n",
       "             ('5ad53ac65b96ef001a10abaf', 'encoding'),\n",
       "             ('56e20a3ae3433e140042324a', 'Manuel Blum'),\n",
       "             ('56e20a3ae3433e140042324b', 'speed-up theorem.'),\n",
       "             ('56e20a3ae3433e140042324d',\n",
       "              '\"Reducibility Among Combinatorial Problems\",'),\n",
       "             ('56e20a3ae3433e140042324e', '21'),\n",
       "             ('5ad572b15b96ef001a10af06', ''),\n",
       "             ('5ad572b15b96ef001a10af07', ''),\n",
       "             ('5ad572b15b96ef001a10af08', ''),\n",
       "             ('5ad572b15b96ef001a10af09', ''),\n",
       "             ('5ad572b15b96ef001a10af0a', ''),\n",
       "             ('5705e26d75f01819005e76d4', 'SoCal'),\n",
       "             ('5705e26d75f01819005e76d5', '10'),\n",
       "             ('5705e26d75f01819005e76d6', 'economic center'),\n",
       "             ('5705e26d75f01819005e76d7', 'demographics and economic'),\n",
       "             ('5705e26d75f01819005e76d8', 'historical political divisions'),\n",
       "             ('5ad0178577cf76001a68698a', ''),\n",
       "             ('5ad0178577cf76001a68698b', 'Kern and San Luis Obispo'),\n",
       "             ('5ad0178577cf76001a68698c', 'Southern California'),\n",
       "             ('5705e33f52bb89140068964c',\n",
       "              'greater Southern California Megaregion'),\n",
       "             ('5705e33f52bb89140068964d', '11'),\n",
       "             ('5705e33f52bb89140068964e', 'Nevada'),\n",
       "             ('5705e33f52bb89140068964f', 'Mexican border into Tijuana'),\n",
       "             ('5705e33f52bb891400689650', 'Tijuana'),\n",
       "             ('5ad0185a77cf76001a6869b8', ''),\n",
       "             ('5ad0185a77cf76001a6869b9', ''),\n",
       "             ('5ad0185a77cf76001a6869ba', ''),\n",
       "             ('5705e3f252bb89140068966a', 'Pacific coast'),\n",
       "             ('5705e3f252bb89140068966b', 'seven'),\n",
       "             ('5705e3f252bb89140068966c', 'over 12 million'),\n",
       "             ('5705e3f252bb89140068966d', ''),\n",
       "             ('5705e3f252bb89140068966e', 'over 17.5 million'),\n",
       "             ('5ad01a6277cf76001a686a0c', '60'),\n",
       "             ('5ad01a6277cf76001a686a0d', ''),\n",
       "             ('5ad01a6277cf76001a686a0e', ''),\n",
       "             ('5ad01a6277cf76001a686a0f', ''),\n",
       "             ('5705e4fe75f01819005e7704', 'Colorado River'),\n",
       "             ('5705e4fe75f01819005e7705', 'Colorado Desert'),\n",
       "             ('5705e4fe75f01819005e7706', 'Mojave Desert'),\n",
       "             ('5705e4fe75f01819005e7707', 'Mexico–United States border.'),\n",
       "             ('5ad01c8877cf76001a686a64', ''),\n",
       "             ('5ad01c8877cf76001a686a65', ''),\n",
       "             ('5ad01c8877cf76001a686a66', ''),\n",
       "             ('5705e63175f01819005e7720', 'southern California'),\n",
       "             ('5705e63175f01819005e7721', '3,792,621,'),\n",
       "             ('5705e63175f01819005e7722', 'Los Angeles'),\n",
       "             ('5705e63175f01819005e7723', 'San Diego'),\n",
       "             ('5705e63175f01819005e7724', 'south'),\n",
       "             ('5ad01e4677cf76001a686aa6', ''),\n",
       "             ('5ad01e4677cf76001a686aa7', ''),\n",
       "             ('5ad01e4677cf76001a686aa8', ''),\n",
       "             ('5ad01e4677cf76001a686aa9', ''),\n",
       "             ('5705e99452bb891400689688', ''),\n",
       "             ('5705e99452bb891400689689', 'United States'),\n",
       "             ('5705e99452bb89140068968a',\n",
       "              'the five most populous in the state'),\n",
       "             ('5705e99452bb89140068968b', ''),\n",
       "             ('5705e99452bb89140068968c', ''),\n",
       "             ('5ad01f0f77cf76001a686ac2',\n",
       "              'Los Angeles, Orange, San Diego, San Bernardino, and Riverside'),\n",
       "             ('5ad01f0f77cf76001a686ac3', '15'),\n",
       "             ('5ad01f0f77cf76001a686ac4', ''),\n",
       "             ('5705eb3375f01819005e7764', 'Hollywood'),\n",
       "             ('5705eb3375f01819005e7765',\n",
       "              'Los Angeles in southern California. Hollywood, a district within Los Angeles,'),\n",
       "             ('5705eb3375f01819005e7766', 'Walt Disney Company'),\n",
       "             ('5705eb3375f01819005e7767', 'music'),\n",
       "             ('5705eb3375f01819005e7768', 'Sony'),\n",
       "             ('5ad0206677cf76001a686afa', ''),\n",
       "             ('5ad0206677cf76001a686afb', ''),\n",
       "             ('5ad0206677cf76001a686afc', ''),\n",
       "             ('5ad0206677cf76001a686afd', ''),\n",
       "             ('5705ec1675f01819005e776e', 'skateboard'),\n",
       "             ('5705ec1675f01819005e776f', 'Tony Hawk'),\n",
       "             ('5705ec1675f01819005e7770', 'Shaun White'),\n",
       "             ('5705ec1675f01819005e7771', 'Oahu'),\n",
       "             ('5705ec1675f01819005e7772', 'Transpac'),\n",
       "             ('5ad0220a77cf76001a686b2a', 'Southern California'),\n",
       "             ('5ad0220a77cf76001a686b2b', ''),\n",
       "             ('5ad0220a77cf76001a686b2c', 'Southern California'),\n",
       "             ('5ad0220a77cf76001a686b2d', ''),\n",
       "             ('5ad0220a77cf76001a686b2e', ''),\n",
       "             ('5705eccb52bb8914006896b8', 'Palm Springs'),\n",
       "             ('5705eccb52bb8914006896b9', ''),\n",
       "             ('5705eccb52bb8914006896ba', 'southern'),\n",
       "             ('5705eccb52bb8914006896bb', 'nearby open spaces'),\n",
       "             ('5ad0228a77cf76001a686b58', ''),\n",
       "             ('5ad0228a77cf76001a686b59', ''),\n",
       "             ('5ad0228a77cf76001a686b5a', ''),\n",
       "             ('5705edcd52bb8914006896ca', '37° 9\\' 58.23\"'),\n",
       "             ('5705edcd52bb8914006896cb', '11'),\n",
       "             ('5705edcd52bb8914006896cc', 'ten'),\n",
       "             ('5705edcd52bb8914006896cd', 'Tehachapi Mountains'),\n",
       "             ('5705edcd52bb8914006896ce', 'Tehachapi Mountains'),\n",
       "             ('5ad0297e77cf76001a686c3a', ''),\n",
       "             ('5ad0297e77cf76001a686c3b', ''),\n",
       "             ('5ad0297e77cf76001a686c3c', ''),\n",
       "             ('5ad0297e77cf76001a686c3d', ''),\n",
       "             ('5705eee952bb8914006896de', 'Mexico'),\n",
       "             ('5705eee952bb8914006896df', 'Alta California'),\n",
       "             ('5705eee952bb8914006896e0', 'Monterey'),\n",
       "             ('5705eee952bb8914006896e1', 'the Compromise'),\n",
       "             ('5705eee952bb8914006896e2', 'free'),\n",
       "             ('5ad02cde77cf76001a686cc6', 'Mexico'),\n",
       "             ('5ad02cde77cf76001a686cc7', ''),\n",
       "             ('5ad02cde77cf76001a686cc8', ''),\n",
       "             ('5ad02cde77cf76001a686cc9', ''),\n",
       "             ('5705f09e75f01819005e77a4', 'taxes'),\n",
       "             ('5705f09e75f01819005e77a5', '\"Cow Counties\"'),\n",
       "             ('5705f09e75f01819005e77a6', 'three'),\n",
       "             ('5705f09e75f01819005e77a7', '75%'),\n",
       "             ('5705f09e75f01819005e77a8', 'Milton Latham'),\n",
       "             ('5ad02e5d77cf76001a686d3a', ''),\n",
       "             ('5ad02e5d77cf76001a686d3b', ''),\n",
       "             ('5ad02e5d77cf76001a686d3c', ''),\n",
       "             ('5ad02e5d77cf76001a686d3d', ''),\n",
       "             ('5ad02e5d77cf76001a686d3e', ''),\n",
       "             ('5705f13d52bb8914006896f0', 'the Los Angeles Times'),\n",
       "             ('5705f13d52bb8914006896f1', '1900'),\n",
       "             ('5705f13d52bb8914006896f2', '1999'),\n",
       "             ('5705f13d52bb8914006896f3', 'county—Imperial—to'),\n",
       "             ('5705f13d52bb8914006896f4', 'seven'),\n",
       "             ('5ad02ee077cf76001a686d58', ''),\n",
       "             ('5ad02ee077cf76001a686d59', ''),\n",
       "             ('5ad02ee077cf76001a686d5a', ''),\n",
       "             ('5705f36452bb891400689718', 'regional tourism groups'),\n",
       "             ('5705f36452bb891400689719',\n",
       "              'California State Automobile Association'),\n",
       "             ('5705f36452bb89140068971a', 'three-region'),\n",
       "             ('5705f36452bb89140068971b', 'transverse range'),\n",
       "             ('5705f36452bb89140068971c', ''),\n",
       "             ('5ad02ff177cf76001a686da2', ''),\n",
       "             ('5ad02ff177cf76001a686da3', ''),\n",
       "             ('5ad02ff177cf76001a686da4', ''),\n",
       "             ('5ad02ff177cf76001a686da5', ''),\n",
       "             ('5705f7c875f01819005e77dc', 'third'),\n",
       "             ('5705f7c875f01819005e77dd', 'vast areas'),\n",
       "             ('5705f7c875f01819005e77de', 'use of automobiles and highways'),\n",
       "             ('5705f7c875f01819005e77df', 'highways'),\n",
       "             ('5705f7c875f01819005e77e0', 'international metropolitan region'),\n",
       "             ('5ad0316a77cf76001a686de4', ''),\n",
       "             ('5ad0316a77cf76001a686de5', ''),\n",
       "             ('5ad0316a77cf76001a686de6', ''),\n",
       "             ('5705fb7f52bb891400689750', 'Camp Pendleton'),\n",
       "             ('5705fb7f52bb891400689751', 'the Inland Empire'),\n",
       "             ('5705fb7f52bb891400689752', ''),\n",
       "             ('5705fb7f52bb891400689753', 'Orange'),\n",
       "             ('5705fb7f52bb891400689754', '1990s'),\n",
       "             ('5ad0331877cf76001a686e16', ''),\n",
       "             ('5ad0331877cf76001a686e17', 'L.A. and Orange Counties,'),\n",
       "             ('5ad0331877cf76001a686e18', ''),\n",
       "             ('5ad0331877cf76001a686e19', 'Santa Maria and San Luis Obispo'),\n",
       "             ('5705fc3a52bb89140068976a', 'Mediterranean'),\n",
       "             ('5705fc3a52bb89140068976b', 'infrequent rain'),\n",
       "             ('5705fc3a52bb89140068976c', ''),\n",
       "             ('5705fc3a52bb89140068976d', 'very rare'),\n",
       "             ('5705fc3a52bb89140068976e', \"70-50's,\"),\n",
       "             ('5ad0381a77cf76001a686e30', 'Mediterranean'),\n",
       "             ('5ad0381a77cf76001a686e31', ''),\n",
       "             ('5ad0381a77cf76001a686e32', ''),\n",
       "             ('5ad0381a77cf76001a686e33', ''),\n",
       "             ('5705fd8475f01819005e7840',\n",
       "              'geologic, topographic, and natural ecosystem landscapes'),\n",
       "             ('5705fd8475f01819005e7841', 'Pacific Ocean'),\n",
       "             ('5705fd8475f01819005e7842', 'topographic'),\n",
       "             ('5705fd8475f01819005e7843', 'Peninsular'),\n",
       "             ('5705fd8475f01819005e7844', ''),\n",
       "             ('5ad0394877cf76001a686e56', ''),\n",
       "             ('5ad0394877cf76001a686e57', ''),\n",
       "             ('5ad0394877cf76001a686e58', ''),\n",
       "             ('5ad0394877cf76001a686e59', ''),\n",
       "             ('5705fec152bb89140068977a', '10,000'),\n",
       "             ('5705fec152bb89140068977b', '10,000'),\n",
       "             ('5705fec152bb89140068977c', '6.7'),\n",
       "             ('5705fec152bb89140068977d', 'property damage'),\n",
       "             ('5705fec152bb89140068977e', 'over $20 billion.'),\n",
       "             ('5ad03c7377cf76001a686ebc', ''),\n",
       "             ('5ad03c7377cf76001a686ebd', ''),\n",
       "             ('5ad03c7377cf76001a686ebe', '10,000'),\n",
       "             ('5ad03c7377cf76001a686ebf', ''),\n",
       "             ('5705ffde52bb891400689784', 'San Andreas Fault'),\n",
       "             ('5705ffde52bb891400689785', '6.7+'),\n",
       "             ('5705ffde52bb891400689786', 'Puente Hills Fault'),\n",
       "             ('5705ffde52bb891400689787', 'The USGS'),\n",
       "             ('5705ffde52bb891400689788', 'Earthquake occurrence'),\n",
       "             ('5ad03d2277cf76001a686ecc', ''),\n",
       "             ('5ad03d2277cf76001a686ecd',\n",
       "              'San Jacinto Fault, the Puente Hills Fault, and the Elsinore Fault Zone.'),\n",
       "             ('5ad03d2277cf76001a686ece', ''),\n",
       "             ('5ad03d2277cf76001a686ecf', ''),\n",
       "             ('570602fa52bb89140068979e', 'economically'),\n",
       "             ('570602fa52bb89140068979f', 'global'),\n",
       "             ('570602fa52bb8914006897a0', 'economic'),\n",
       "             ('5ad0410977cf76001a686efe', ''),\n",
       "             ('5ad0410977cf76001a686eff', 'hub of economic activity'),\n",
       "             ('5ad0410977cf76001a686f00',\n",
       "              'culturally, politically, and economically into distinctive regions,'),\n",
       "             ('570603c475f01819005e7882', '2010'),\n",
       "             ('570603c475f01819005e7883', 'high growth rates'),\n",
       "             ('570603c475f01819005e7884', '10.0%'),\n",
       "             ('570603c475f01819005e7885', ''),\n",
       "             ('570603c475f01819005e7886', 'Greater Sacramento'),\n",
       "             ('5ad0421177cf76001a686f14', ''),\n",
       "             ('5ad0421177cf76001a686f15', ''),\n",
       "             ('5ad0421177cf76001a686f16', ''),\n",
       "             ('5ad0421177cf76001a686f17', ''),\n",
       "             ('5706074552bb8914006897d4', ''),\n",
       "             ('5706074552bb8914006897d5', 'two'),\n",
       "             ('5706074552bb8914006897d6', 'five million'),\n",
       "             ('5706074552bb8914006897d7', 'the Southern Border Region'),\n",
       "             ('5706074552bb8914006897d8', '17,786,419,'),\n",
       "             ('5ad042cf77cf76001a686f24', ''),\n",
       "             ('5ad042cf77cf76001a686f25', ''),\n",
       "             ('5ad042cf77cf76001a686f26', ''),\n",
       "             ('5ad042cf77cf76001a686f27', ''),\n",
       "             ('5ad042cf77cf76001a686f28', ''),\n",
       "             ('570607f575f01819005e78b4', 'Los Angeles'),\n",
       "             ('570607f575f01819005e78b5', ''),\n",
       "             ('570607f575f01819005e78b6', ''),\n",
       "             ('570607f575f01819005e78b7', '100,000'),\n",
       "             ('570607f575f01819005e78b8', 'Riverside'),\n",
       "             ('5ad0440877cf76001a686f3e', ''),\n",
       "             ('5ad0440877cf76001a686f3f', ''),\n",
       "             ('5ad0440877cf76001a686f40', ''),\n",
       "             ('5ad0440877cf76001a686f41', ''),\n",
       "             ('5ad0440877cf76001a686f42', ''),\n",
       "             ('5706094b52bb8914006897de', 'petroleum'),\n",
       "             ('5706094b52bb8914006897df', 'Hollywood'),\n",
       "             ('5706094b52bb8914006897e0', 'housing bubble'),\n",
       "             ('5706094b52bb8914006897e1', 'diverse'),\n",
       "             ('5706094b52bb8914006897e2', 'heavily impacted'),\n",
       "             ('5ad044f077cf76001a686f58', ''),\n",
       "             ('5ad044f077cf76001a686f59', ''),\n",
       "             ('5ad044f077cf76001a686f5a', 'abundance of petroleum'),\n",
       "             ('5ad044f077cf76001a686f5b', ''),\n",
       "             ('57060a1175f01819005e78d2', '1920s'),\n",
       "             ('57060a1175f01819005e78d3',\n",
       "              'richest agricultural regions in the U.S., cattle and citrus were major industries until farmlands were turned into suburbs.'),\n",
       "             ('57060a1175f01819005e78d4', 'cattle'),\n",
       "             ('57060a1175f01819005e78d5', 'citrus'),\n",
       "             ('57060a1175f01819005e78d6', ''),\n",
       "             ('5ad0458b77cf76001a686f72', ''),\n",
       "             ('5ad0458b77cf76001a686f73', ''),\n",
       "             ('5ad0458b77cf76001a686f74', ''),\n",
       "             ('5ad0458b77cf76001a686f75', ''),\n",
       "             ('57060a6e52bb8914006897f8', 'business districts'),\n",
       "             ('57060a6e52bb8914006897f9', 'Central business districts'),\n",
       "             ('57060a6e52bb8914006897fa', ''),\n",
       "             ('5ad0461477cf76001a686f7a', 'Central business districts'),\n",
       "             ('5ad0461477cf76001a686f7b', 'Central business districts'),\n",
       "             ('5ad0461477cf76001a686f7c', ''),\n",
       "             ('57060cc352bb89140068980e', 'business'),\n",
       "             ('57060cc352bb89140068980f', 'Los Angeles'),\n",
       "             ('57060cc352bb891400689810', 'San Fernando Valley'),\n",
       "             ('57060cc352bb891400689811', 'Los Angeles'),\n",
       "             ('5ad0470477cf76001a686f80',\n",
       "              'Downtown Burbank, Downtown Santa Monica, Downtown Glendale and Downtown Long Beach.'),\n",
       "             ('5ad0470477cf76001a686f81',\n",
       "              'Downtown Los Angeles central business district'),\n",
       "             ('5ad0470477cf76001a686f82',\n",
       "              'Downtown Los Angeles central business district'),\n",
       "             ('57060df252bb891400689820', 'business'),\n",
       "             ('57060df252bb891400689821', 'Downtown Riverside'),\n",
       "             ('57060df252bb891400689822',\n",
       "              'Hospitality Business/Financial Centre,'),\n",
       "             ('5ad0475c77cf76001a686f86',\n",
       "              'Downtown San Bernardino, Hospitality Business/Financial Centre, University Town'),\n",
       "             ('5ad0475c77cf76001a686f87', ''),\n",
       "             ('57060eaf75f01819005e7910', 'Orange County'),\n",
       "             ('57060eaf75f01819005e7911', 'University of California, Irvine.'),\n",
       "             ('57060eaf75f01819005e7912', 'Irvine Tech Center'),\n",
       "             ('57060eaf75f01819005e7913', 'South Coast Metro'),\n",
       "             ('57060eaf75f01819005e7914', 'rapidly'),\n",
       "             ('5ad0483977cf76001a686f8a', 'Orange County'),\n",
       "             ('5ad0483977cf76001a686f8b', ''),\n",
       "             ('5ad0483977cf76001a686f8c', 'international corporations'),\n",
       "             ('5ad0483977cf76001a686f8d', ''),\n",
       "             ('57060f3e75f01819005e7922', 'Downtown San Diego'),\n",
       "             ('57060f3e75f01819005e7923', 'North County regions'),\n",
       "             ('57060f3e75f01819005e7924', 'North County'),\n",
       "             ('57060f3e75f01819005e7925', 'San Diego'),\n",
       "             ('5ad04b2377cf76001a686f92', ''),\n",
       "             ('5ad04b2377cf76001a686f93', ''),\n",
       "             ('5ad04b2377cf76001a686f94', ''),\n",
       "             ('570610b275f01819005e792a', 'Los Angeles International Airport'),\n",
       "             ('570610b275f01819005e792b',\n",
       "              \"World's busiest airports by passenger traffic)\"),\n",
       "             ('570610b275f01819005e792c', 'third'),\n",
       "             ('570610b275f01819005e792d', 'San Diego International Airport'),\n",
       "             ('570610b275f01819005e792e', 'Van Nuys Airport'),\n",
       "             ('5ad04bfc77cf76001a686f98', ''),\n",
       "             ('5ad04bfc77cf76001a686f99', ''),\n",
       "             ('5ad04bfc77cf76001a686f9a', ''),\n",
       "             ('5ad04bfc77cf76001a686f9b', ''),\n",
       "             ('5706111a52bb89140068984c', 'Metrolink'),\n",
       "             ('5706111a52bb89140068984d', 'seven'),\n",
       "             ('5706111a52bb89140068984e', 'Six'),\n",
       "             ('5706111a52bb89140068984f', 'Orange'),\n",
       "             ('5ad04d3077cf76001a686fa0', 'Downtown Los Angeles'),\n",
       "             ('5ad04d3077cf76001a686fa1', 'Metrolink'),\n",
       "             ('5ad04d3077cf76001a686fa2', 'Riverside, Orange, and San Diego'),\n",
       "             ('570611c475f01819005e793c', 'Port of Los Angeles'),\n",
       "             ('570611c475f01819005e793d', 'Port of Long Beach'),\n",
       "             ('570611c475f01819005e793e', 'Southern'),\n",
       "             ('5ad04de377cf76001a686fa6', ''),\n",
       "             ('5ad04de377cf76001a686fa7', 'Port of Long Beach'),\n",
       "             ('5ad04de377cf76001a686fa8', ''),\n",
       "             ('5ad04de377cf76001a686fa9', 'Port of Long Beach'),\n",
       "             ('5706139252bb891400689864', 'The Tech Coast'),\n",
       "             ('5706139252bb891400689865',\n",
       "              'prestigious and world-renowned research universities'),\n",
       "             ('5706139252bb891400689866', 'private'),\n",
       "             ('5706139252bb891400689867', '5'),\n",
       "             ('5706139252bb891400689868', '12'),\n",
       "             ('5ad04ecf77cf76001a686fae', ''),\n",
       "             ('5ad04ecf77cf76001a686faf', '12'),\n",
       "             ('5ad04ecf77cf76001a686fb0', '12'),\n",
       "             ('5ad04ecf77cf76001a686fb1', ''),\n",
       "             ('5ad04ecf77cf76001a686fb2', ''),\n",
       "             ('5706143575f01819005e7950',\n",
       "              'Professional sports teams in Southern California include teams from the NFL'),\n",
       "             ('5706143575f01819005e7951', 'NBA'),\n",
       "             ('5706143575f01819005e7952', 'MLB'),\n",
       "             ('5706143575f01819005e7953', '(Los Angeles Kings,'),\n",
       "             ('5706143575f01819005e7954', '(LA Galaxy).'),\n",
       "             ('5ad04f7977cf76001a686fb8',\n",
       "              '(Los Angeles Rams, San Diego Chargers);'),\n",
       "             ('5ad04f7977cf76001a686fb9', ''),\n",
       "             ('5ad04f7977cf76001a686fba',\n",
       "              '(Los Angeles Rams, San Diego Chargers);'),\n",
       "             ('5ad04f7977cf76001a686fbb',\n",
       "              '(Los Angeles Rams, San Diego Chargers);'),\n",
       "             ('5ad04f7977cf76001a686fbc', 'MLS (LA Galaxy).'),\n",
       "             ('5706149552bb891400689880', 'Chivas USA'),\n",
       "             ('5706149552bb891400689881', 'two'),\n",
       "             ('5706149552bb891400689882', '2014'),\n",
       "             ('5706149552bb891400689883', 'StubHub Center'),\n",
       "             ('5706149552bb891400689884', '2018'),\n",
       "             ('5ad050c877cf76001a686fc2', 'two'),\n",
       "             ('5ad050c877cf76001a686fc3', 'the LA Galaxy and Chivas USA'),\n",
       "             ('5ad050c877cf76001a686fc4', ''),\n",
       "             ('5ad050c877cf76001a686fc5', ''),\n",
       "             ('570614ff52bb89140068988a', 'College sports'),\n",
       "             ('570614ff52bb89140068988b', 'UCLA'),\n",
       "             ('570614ff52bb89140068988c', 'USC Trojans'),\n",
       "             ('570614ff52bb89140068988d', 'Pac-12'),\n",
       "             ('570614ff52bb89140068988e', 'NCAA Division I'),\n",
       "             ('5ad0511577cf76001a686fca', 'UCLA Bruins'),\n",
       "             ('5ad0511577cf76001a686fcb', 'UCLA Bruins'),\n",
       "             ('5ad0511577cf76001a686fcc', 'Pac-12'),\n",
       "             ('5706155352bb891400689894', 'Rugby'),\n",
       "             ('5706155352bb891400689895', 'high school'),\n",
       "             ('5706155352bb891400689896', 'official school sport'),\n",
       "             ('5ad051be77cf76001a686fd0', ''),\n",
       "             ('57092322efce8f15003a7db0', 'BSkyB'),\n",
       "             ('57092322efce8f15003a7db1', 'BSkyB'),\n",
       "             ('57092322efce8f15003a7db2', '2014'),\n",
       "             ('57092322efce8f15003a7db3',\n",
       "              'British Sky Broadcasting Group plc'),\n",
       "             ('57092322efce8f15003a7db4', 'Sky UK Limited'),\n",
       "             ('5a2c08aebfd06b001a5ae989', ''),\n",
       "             ('5a2c08aebfd06b001a5ae98a', ''),\n",
       "             ('5a2c08aebfd06b001a5ae98b', ''),\n",
       "             ('5a2c08aebfd06b001a5ae98c', ''),\n",
       "             ('5a2c08aebfd06b001a5ae98d', ''),\n",
       "             ('57094a79efce8f15003a7dc4', '2006'),\n",
       "             ('57094a79efce8f15003a7dc5', ''),\n",
       "             ('57094a79efce8f15003a7dc6', 'Sky'),\n",
       "             ('57094a79efce8f15003a7dc7', '£1.3bn.'),\n",
       "             ('5a2c0c2fbfd06b001a5ae99b', ''),\n",
       "             ('5a2c0c2fbfd06b001a5ae99c', ''),\n",
       "             ('5a2c0c2fbfd06b001a5ae99d', ''),\n",
       "             ('5a2c0c2fbfd06b001a5ae99e', ''),\n",
       "             ('5a2c0c2fbfd06b001a5ae99f', ''),\n",
       "             ('57094b4f9928a814004714f6', 'ONdigital'),\n",
       "             ('57094b4f9928a814004714f7', 'ITV Digital'),\n",
       "             ('57094b4f9928a814004714f8', 'three'),\n",
       "             ('57094b4f9928a814004714f9', 'Sky Three'),\n",
       "             ('57094b4f9928a814004714fa', ''),\n",
       "             ('5a2c14b4bfd06b001a5ae9d3', ''),\n",
       "             ('5a2c14b4bfd06b001a5ae9d4', ''),\n",
       "             ('5a2c14b4bfd06b001a5ae9d5', ''),\n",
       "             ('5a2c14b4bfd06b001a5ae9d6', ''),\n",
       "             ('5a2c14b4bfd06b001a5ae9d7', ''),\n",
       "             ('57094ca7efce8f15003a7dd6', 'Sky+ PVR'),\n",
       "             ('57094ca7efce8f15003a7dd7', 'September 2007'),\n",
       "             ('57094ca7efce8f15003a7dd8', 'a monthly fee'),\n",
       "             ('57094ca7efce8f15003a7dd9', 'January 2010'),\n",
       "             ('57094ca7efce8f15003a7dda', 'BSkyB'),\n",
       "             ('5a2c16e3bfd06b001a5ae9e7', ''),\n",
       "             ('5a2c16e3bfd06b001a5ae9e8', ''),\n",
       "             ('5a2c16e3bfd06b001a5ae9e9', ''),\n",
       "             ('5a2c16e3bfd06b001a5ae9ea', ''),\n",
       "             ('5a2c16e3bfd06b001a5ae9eb', ''),\n",
       "             ('57094d489928a8140047150a', 'VideoGuard'),\n",
       "             ('57094d489928a8140047150b', ''),\n",
       "             ('57094d489928a8140047150c', 'Cisco Systems'),\n",
       "             ('57094d489928a8140047150d', 'BSkyB'),\n",
       "             ('57094d489928a8140047150e', 'Sky+).'),\n",
       "             ('5a2c3a5cbfd06b001a5aea79', ''),\n",
       "             ('5a2c3a5cbfd06b001a5aea7a', ''),\n",
       "             ('5a2c3a5cbfd06b001a5aea7b', ''),\n",
       "             ('5a2c3a5cbfd06b001a5aea7c', ''),\n",
       "             ('5a2c3a5cbfd06b001a5aea7d', ''),\n",
       "             ('570953a7efce8f15003a7dfe', '2007'),\n",
       "             ...])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "70b89ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "48229d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'56ddde6b9a695914005b9628'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.qas_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0a7b98f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qas_id': '56ddde6b9a695914005b9628', 'question_text': 'In what country is Normandy located?', 'context_text': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.', 'answer_text': None, 'title': 'Normans', 'is_impossible': False, 'answers': [{'text': 'France', 'answer_start': 159}, {'text': 'France', 'answer_start': 159}, {'text': 'France', 'answer_start': 159}, {'text': 'France', 'answer_start': 159}], 'start_position': 0, 'end_position': 0, 'doc_tokens': ['The', 'Normans', '(Norman:', 'Nourmands;', 'French:', 'Normands;', 'Latin:', 'Normanni)', 'were', 'the', 'people', 'who', 'in', 'the', '10th', 'and', '11th', 'centuries', 'gave', 'their', 'name', 'to', 'Normandy,', 'a', 'region', 'in', 'France.', 'They', 'were', 'descended', 'from', 'Norse', '(\"Norman\"', 'comes', 'from', '\"Norseman\")', 'raiders', 'and', 'pirates', 'from', 'Denmark,', 'Iceland', 'and', 'Norway', 'who,', 'under', 'their', 'leader', 'Rollo,', 'agreed', 'to', 'swear', 'fealty', 'to', 'King', 'Charles', 'III', 'of', 'West', 'Francia.', 'Through', 'generations', 'of', 'assimilation', 'and', 'mixing', 'with', 'the', 'native', 'Frankish', 'and', 'Roman-Gaulish', 'populations,', 'their', 'descendants', 'would', 'gradually', 'merge', 'with', 'the', 'Carolingian-based', 'cultures', 'of', 'West', 'Francia.', 'The', 'distinct', 'cultural', 'and', 'ethnic', 'identity', 'of', 'the', 'Normans', 'emerged', 'initially', 'in', 'the', 'first', 'half', 'of', 'the', '10th', 'century,', 'and', 'it', 'continued', 'to', 'evolve', 'over', 'the', 'succeeding', 'centuries.'], 'char_to_word_offset': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 14, 14, 15, 15, 15, 15, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 42, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 44, 44, 44, 44, 44, 45, 45, 45, 45, 45, 45, 46, 46, 46, 46, 46, 46, 47, 47, 47, 47, 47, 47, 47, 48, 48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 51, 51, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 53, 53, 53, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 56, 57, 57, 57, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 60, 60, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 62, 62, 62, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 64, 64, 64, 64, 65, 65, 65, 65, 65, 65, 65, 66, 66, 66, 66, 66, 67, 67, 67, 67, 68, 68, 68, 68, 68, 68, 68, 69, 69, 69, 69, 69, 69, 69, 69, 69, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 75, 75, 75, 75, 75, 75, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78, 79, 79, 79, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81, 81, 81, 81, 82, 82, 82, 83, 83, 83, 83, 83, 84, 84, 84, 84, 84, 84, 84, 84, 84, 85, 85, 85, 85, 86, 86, 86, 86, 86, 86, 86, 86, 86, 87, 87, 87, 87, 87, 87, 87, 87, 87, 88, 88, 88, 88, 89, 89, 89, 89, 89, 89, 89, 90, 90, 90, 90, 90, 90, 90, 90, 90, 91, 91, 91, 92, 92, 92, 92, 93, 93, 93, 93, 93, 93, 93, 93, 94, 94, 94, 94, 94, 94, 94, 94, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 96, 96, 96, 97, 97, 97, 97, 98, 98, 98, 98, 98, 98, 99, 99, 99, 99, 99, 100, 100, 100, 101, 101, 101, 101, 102, 102, 102, 102, 102, 103, 103, 103, 103, 103, 103, 103, 103, 103, 104, 104, 104, 104, 105, 105, 105, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 107, 107, 107, 108, 108, 108, 108, 108, 108, 108, 109, 109, 109, 109, 109, 110, 110, 110, 110, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 112, 112, 112, 112, 112, 112, 112, 112, 112, 112]}\n"
     ]
    }
   ],
   "source": [
    "print(vars(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d59a4983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in what country is normandy located?\n",
      "France\n"
     ]
    }
   ],
   "source": [
    "question = example.question_text.lower()\n",
    "context  = example.context_text\n",
    "victims = ['what', 'who', 'which', 'when', 'why', 'how', 'where']\n",
    "pred_ans = predictions[example.qas_id]\n",
    "print(question_text)\n",
    "print(pred_ans)\n",
    "for victim in victims:\n",
    "    if(question.find(victim) >0 ):\n",
    "        statement = question.replace(victim, pred_ans)\n",
    "        statement = statement.replace(\"?\", \".\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0c2c45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_ans = bool(example.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "292fed15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in France country is normandy located.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da18023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37cf702",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = squad_evaluate(examples, predictions)\n",
    "result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n",
    "logger.info(\"Results: {}\".format(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bert]",
   "language": "python",
   "name": "conda-env-bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
